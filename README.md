#
https://www.twman.org/AI/CV

https://huggingface.co/DeepLearning101
#

<details open>
<summary><strong>æ‰‹æŠŠæ‰‹å¸¶ä½ ä¸€èµ·è¸©AIå‘ï¼šhttps://reurl.cc/g6GlZX</strong></summary>
   
- [ç™½è©±æ–‡æ‰‹æŠŠæ‰‹å¸¶ä½ ç§‘æ™® GenAI](https://blog.twman.org/2024/08/LLM.html)
   
- [ComfyUI + Stable Diffuision](https://blog.twman.org/2024/11/diffusion.html)
  
- [å¤§å‹èªè¨€æ¨¡å‹ç›´æ¥å°±æ‰“å®Œæ”¶å·¥ï¼Ÿ](https://blog.twman.org/2024/09/LLM.html)
  
- [é‚£äº›æª¢ç´¢å¢å¼·ç”Ÿæˆè¦è¸©çš„å‘](https://blog.twman.org/2024/07/RAG.html)
  
- [é‚£äº›å¤§å‹èªè¨€æ¨¡å‹è¦è¸©çš„å‘](https://blog.twman.org/2024/02/LLM.html)
  
- [Large Language Modelï¼ŒLLM](https://blog.twman.org/2023/04/GPT.html)
  
- [é‚£äº›è‡ªç„¶èªè¨€è™•ç†è¸©çš„å‘](https://blog.twman.org/2021/04/NLP.html)

- [é‚£äº›èªéŸ³è™•ç† (Speech Processing) è¸©çš„å‘](https://blog.twman.org/2021/04/ASR.html)

- [é‚£äº›ASRå’ŒTTSå¯èƒ½æœƒè¸©çš„å‘](https://blog.twman.org/2024/02/asr-tts.html)
</details>

# Computer-Vision (é›»è…¦è¦–è¦º)


## Segmentation (åœ–åƒåˆ†å‰²)

- [Meta Segment Anything Model 2 (SAM 2)](https://ai.meta.com/sam2/)
   - [60è¡Œç¨‹å¼ç¢¼è¨“ç·´/å¾®èª¿Segment Anything 2](https://mp.weixin.qq.com/s/YfgYCzvi0cXxOFIfQvE_9w)
   - [CLIPSegï¼šImage Segmentation Using Text and Image Prompts](https://github.com/timojl/clipseg)ï¼š[Huggingface Space](https://huggingface.co/spaces/taesiri/CLIPSeg)
      - [å“¥å»·æ ¹å¤§å­¸æå‡ºCLIPSegï¼Œèƒ½åŒæ™‚ä½œä¸‰å€‹åˆ†å‰²ä»»å‹™çš„æ¨¡å‹](https://mp.weixin.qq.com/s/evKssKulZiUssLN71t6_Lw)
      - [SAMèˆ‡CLIPå¼·å¼·è¯æ‰‹ï¼Œå¯¦ç¾22000é¡çš„åˆ†å‰²èˆ‡è­˜åˆ¥](https://mp.weixin.qq.com/s/evKssKulZiUssLN71t6_Lw)
- [SAMURAI](https://yangchris11.github.io/samurai/)
   - [ç„¡éœ€è¨“ç·´æˆ–å¾®èª¿å³å¯å¾—åˆ°ç©©å®šã€æº–ç¢ºçš„è¿½è¹¤æ•ˆæœï¼ KF + SAM2 è§£æ±ºå¿«é€Ÿç§»å‹•æˆ–è‡ªé®æ“‹çš„ç‰©ä»¶è¿½è¹¤å•é¡Œ](https://mp.weixin.qq.com/s/iU3Bk_uO01GWUxAtIBsrWQ)
   - [ç¶“å…¸å¡çˆ¾æ›¼æ¿¾æ³¢å™¨æ”¹é€²å½±ç‰‡ç‰ˆã€Œåˆ†å‰²ä¸€åˆ‡ã€ï¼Œç¶²å‹ï¼šå¥½å„ªé›…çš„æ–¹æ³•](https://www.qbitai.com/2024/11/223020.html)
- [Grounded SAM 2: Ground and Track Anything in Videos](https://github.com/IDEA-Research/Grounded-SAM-2)
   - [Grounded-Segment-Anything](https://huggingface.co/spaces/yizhangliu/Grounded-Segment-Anything)
- [SAM2Long](https://github.com/Mark12Ding/SAM2Long)ï¼š[å¤§å¹…æå‡SAM 2æ€§èƒ½ï¼æ¸¯ä¸­æ–‡æå‡ºSAM2Longï¼Œè¤‡é›œé•·è¦–é »çš„åˆ†å‰²æ¨¡å‹](https://mp.weixin.qq.com/s/henvaxGoNgx24NLQV1Qj2w)
- [SAM2-Adapter](https://github.com/tianrun-chen/SAM-Adapter-PyTorch)ï¼š[SAM 2ç„¡æ³•åˆ†å‰²ä¸€åˆ‡ï¼Ÿ SAM2-Adapterï¼šé¦–æ¬¡è®“SAM 2åœ¨ä¸‹æ¸¸ä»»å‹™é©æ‡‰èª¿æ ¡ï¼](https://mp.weixin.qq.com/s/3z-LshKAgbSzNCzyoLOuag)
- [SAM2Point](https://github.com/ZiyuGuo99/SAM2Point)ï¼š[å¯æç¤º3D åˆ†å‰²ç ”ç©¶é‡Œç¨‹ç¢‘ï¼ SAM2Pointï¼šSAM2åŠ æŒå¯æ³›åŒ–ä»»3Då ´æ™¯ã€ä»»æ„æç¤ºï¼](https://mp.weixin.qq.com/s/TnTK5UE7O_hcrNzloxBmAw)



## Diffusion model (æ“´æ•£æ¨¡å‹)

- 2025-02-25ï¼š[Wan-Video](https://github.com/Wan-Video/Wan2.1)ï¼š[è¶…è¶ŠSoraï¼é˜¿é‡Œè¬ç›¸å¤§æ¨¡å‹æ­£å¼é–‹æºï¼å…¨æ¨¡æ…‹ã€å…¨å°ºå¯¸å¤§æ¨¡å‹é–‹æº](https://finance.sina.com.cn/jjxw/2025-02-26/doc-inemukxr9127437.shtml)
- 2025-02-14ï¼š[FlashVideo](https://github.com/FoundationVision/FlashVideo)ï¼š[ä¾†è‡ªä½å…ƒçµ„çš„è¦–è¨Šå¢å¼·å…¨æ–°é–‹æºæ¼”ç®—æ³•ï¼Œ102ç§’ç”¢ç”Ÿ1080Pè¦–é »](https://zhuanlan.zhihu.com/p/23702953115)
- 2025-01-28ï¼š[Sana](https://github.com/NVlabs/Sana)ï¼š[ICLR 2025 Oral] Efficient High-Resolution Image Synthesis with Linear Diffusion Transformerï¼›[æ¯”FLUXå¿«100å€ï¼è‹±å‰é”è¯æ‰‹MITã€æ¸…è¯é–‹æºè¶…å¿«AIå½±åƒç”¢ç”Ÿæ¨¡å‹](https://zhuanlan.zhihu.com/p/19489214543)
- [Flux](https://huggingface.co/black-forest-labs)
   - [Flux.1-canny-dev](https://huggingface.co/spaces/black-forest-labs/FLUX.1-canny-dev)ï¼š[https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev/](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev/)
   - [Flux.1-depth-dev](https://huggingface.co/spaces/black-forest-labs/FLUX.1-Depth-dev)ï¼š[https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev/](https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev/)
   - [Flux.1-fill-dev](https://huggingface.co/spaces/black-forest-labs/FLUX.1-Fill-dev)ï¼š[https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev/](https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev/)
   - [Flux.1-redux-dev](https://huggingface.co/spaces/black-forest-labs/FLUX.1-Redux-dev)ï¼š[https://huggingface.co/black-forest-labs/FLUX.1-Redux-dev/](https://huggingface.co/black-forest-labs/FLUX.1-Redux-dev/)
      - 2024-11-26ï¼š[Fluxå®˜æ–¹é‡ç¹ª+æ“´åœ–+é¢¨æ ¼åƒè€ƒ+ControlNet](https://mp.weixin.qq.com/s/Kj1nyJNTpoZ94JjO4FMw_g)
      - 2024-11-25ï¼š[æœ€æ–°flux_fill_inpaintæ¨¡å‹é«”é©—ã€‚](https://mp.weixin.qq.com/s/OPknDJXH1_oezSR86c_png)
- 2024-12-17ï¼š[Leffa](https://github.com/franciszzj/Leffa)ï¼š[Leffaï¼šMeta AI é–‹æºç²¾ç¢ºæ§åˆ¶äººç‰©å¤–è§€å’Œå§¿å‹¢çš„åœ–åƒç”Ÿæˆæ¡†æ¶ï¼Œåœ¨ç”Ÿæˆç©¿è‘—çš„åŒæ™‚ä¿æŒäººç‰©ç‰¹å¾µ](https://juejin.cn/post/7449325873725276196)
- 2024-11-29ï¼š[PuLID, Pure and Lightning ID Customization via Contrastive Alignment](https://github.com/ToTheBeginning/PuLID)ï¼š[https://github.com/balazik/ComfyUI-PuLID-Flux](https://github.com/balazik/ComfyUI-PuLID-Flux)
   - 2024-11-07ï¼š[æå®šComfyUI-PuLID-Fluxç¯€é»åªè¦é€™å¹¾æ­¥ï¼é™„ä¸€éµå£“ç¸®åŒ…](https://mp.weixin.qq.com/s/07BMFHaSasl7-PFtkN6_Zg)
   - 2024-10-08ï¼š[ä¸€æ–‡ææ‡‚PuLID FLUXäººç‰©æ›è‡‰&é¢¨æ ¼é·ç§»](https://mp.weixin.qq.com/s/V-2Cp8_xFnHQNFn35aGdLg)
- 2024-11-26ï¼š[MagicQuill](https://github.com/magic-quill/MagicQuill)ï¼š[https://huggingface.co/spaces/AI4Editing/MagicQuill](https://huggingface.co/spaces/AI4Editing/MagicQuill)
   - [MagicQuillï¼Œç™»ä¸ŠHuggingfaceè¶¨å‹¢æ¦œæ¦œé¦–çš„AI Påœ–ç¥å™¨](https://mp.weixin.qq.com/s/Pc3xRP8_9BxkVSRNznkplw)
- 2024-11-26ï¼š[OOTDiffusion](https://github.com/levihsu/OOTDiffusion)ï¼š[https://huggingface.co/spaces/levihsu/OOTDiffusion](https://huggingface.co/spaces/levihsu/OOTDiffusion)
   - [é–‹æºAIæ›è£ç¥å™¨OOTDiffusion](https://mp.weixin.qq.com/s/B2rNCjJLo8coYzoHGPnVaw)
- 2024-11-24ï¼š[Comfyui Impact Pack](https://github.com/ltdrdata/ComfyUI-Impact-Pack)
   - [Comfyui æœ€å¼·è‡‰éƒ¨ä¿®å¾©å·¥å…·Impact Pack](https://mp.weixin.qq.com/s/hNQ9BfdGbRQ_Osus-yMJWg)
- 2024-11-05ï¼š[ComfyUI OmniGen @ åŒ—äº¬äººå·¥æ™ºæ…§ç ”ç©¶é™¢](https://github.com/AIFSH/OmniGen-ComfyUI)ï¼š[https://huggingface.co/spaces/Shitao/OmniGen](https://huggingface.co/spaces/Shitao/OmniGen)
   - [ComfyUI å½±åƒç”Ÿæˆæ¨¡å‹OmniGenï¼Œäººç‰©ä¸€è‡´æ€§è™•ç†çš„ä¹Ÿå¤ªå¥½äº†](https://mp.weixin.qq.com/s/msGK0FmNs3T3jbUBHfR9DA)
   - [å…¨èƒ½å½±åƒç”Ÿæˆæ¨¡å‹OmniGenï¼šå‘Šåˆ¥ControlNetã€ipadapterç­‰æ’ä»¶ï¼Œåƒ…æ†‘æç¤ºå³å¯æ§åˆ¶å½±åƒç”Ÿæˆèˆ‡ç·¨è¼¯](https://mp.weixin.qq.com/s/48HmqRGBOK1uBdzlprdKSA)


## Digital Human (è™›æ“¬æ•¸å­—äºº)
- [Linly-Talker](https://github.com/Kedreamix/Linly-Talker)ï¼šan intelligent AI system that combines large language models (LLMs) with visual models to create a novel human-AI interaction method. 
- [EchoMimicV2](https://github.com/antgroup/echomimic_v2)ï¼š[CVPR 2025] EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation
- [Hallo3](https://github.com/fudan-generative-vision/hallo3)ï¼š[CVPR 2025] Highly Dynamic and Realistic Portrait Image Animation with Diffusion Transformer Networks
- [MimicTalk](https://github.com/yerfor/MimicTalk)ï¼š[NeurIPS 2024] MimicTalk: Mimicking a personalized and expressive 3D talking face in minutes
- [JoyGen](https://github.com/JOY-MM/JoyGen)ï¼šAudio-Driven 3D Depth-Aware Talking-Face Video Editing
- [Latentsync](https://github.com/bytedance/LatentSync)
- [MuseTalk](https://github.com/TMElyralab/MuseTalk)



## Image Recognition (åœ–åƒè­˜åˆ¥)

- [ViTï¼ˆVision Transformerï¼‰è§£æ](https://zhuanlan.zhihu.com/p/445122996)ï¼šhttps://github.com/google-research/vision_transformer

- [2040å¼µåœ–ç‰‡è¨“ç·´å‡ºçš„ViTï¼Œæº–ç¢ºç‡96.7%ï¼Œé€£é·ç§»è¡¨ç¾éƒ½ä»¤äººé©šè¨](https://zhuanlan.zhihu.com/p/463608959)

- [Swin Transformer: ç”¨CNNçš„æ–¹å¼æ‰“æ•—CNN](https://zhuanlan.zhihu.com/p/362690149)ï¼šhttps://github.com/microsoft/Swin-Transformer

- [EfficientNetV2éœ‡æ’¼ç™¼å¸ƒï¼æ›´å°çš„æ¨¡å‹ï¼Œæ›´å¿«çš„è¨“ç·´](https://zhuanlan.zhihu.com/p/361873583)ï¼šhttps://github.com/d-li14/efficientnetv2.pytorch

## Optical Character Recognition (å…‰å­¸æ–‡å­—è­˜åˆ¥)

**[é‡å°ç‰©ä»¶æˆ–å ´æ™¯å½±åƒé€²è¡Œåˆ†æèˆ‡åµæ¸¬](https://www.twman.org/AI/CV)**
- 2025-03-03ï¼š[olmocr](https://github.com/allenai/olmocr)ï¼š[ğŸš€æœ¬åœ°éƒ¨ç½²æœ€å¼ºOCRå¤§æ¨¡å‹olmOCRï¼æ”¯æŒç»“æ„åŒ–ç²¾å‡†æå–å¤æ‚PDFæ–‡ä»¶å†…å®¹ï¼](https://www.aivi.fyi/llms/deploy-olmOCR)
- 2025-02-05ï¼š[MinerU](https://github.com/opendatalab/MinerU)ï¼š[å°‡PDFè½‰æ›ç‚ºæ©Ÿå™¨å¯è®€æ ¼å¼çš„ç¥å™¨](https://mp.weixin.qq.com/s/ci5wp6gICTCtaRZfn5yWUQ)
- 2024-12-15ï¼š[markitdown](https://github.com/microsoft/markitdown)
- 2024-09-22ï¼š[OCR2.0æ—¶ä»£-GOTæ¥å•¦ï¼](https://mp.weixin.qq.com/s/W-Ult-F3pU6Wvx3fHEN8yA)
- 2024-09-11ï¼š[GOT-OCR-2.0æ¨¡å‹å¼€æº](https://mp.weixin.qq.com/s/rQL-Q0TGhT6e8Ti4zZalrg)
- 2024-08-20ï¼š[è¬ç‰©çš†å¯AIåŒ–ï¼å‰›é–‹æºå°±æœ‰12000äººåœè§€çš„OCR æƒæPDF é–‹æºå·¥å…·ï¼é‚„å¯è½‰æ›ç‚ºMarkDownï¼](https://www.53ai.com/news/MultimodalLargeModel/2024082059736.html)
- [advancedliteratemachinery/OCR/OmniParser](https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/OmniParser)
- 2024-10-29ï¼š[Alibabaå‡ºå“:OmniParseré€šç”¨æ–‡æª”è¤‡é›œå ´æ™¯ä¸‹OCRæŠ½å–](https://mp.weixin.qq.com/s/_1Aatpna7poIVRhfYk4aAQ)
- [RapidOCR](https://github.com/RapidAI/RapidOCR/blob/main/docs/README_zh.md)
- [12å€‹æµè¡Œçš„é–‹æºå…è²»OCRé …ç›®](https://mp.weixin.qq.com/s/7EuhnQedAX6injBL_Dg_sQ)
- [ç”¨PaddleOCRçš„PPOCRLabelä¾†å¾®èª¿é†«ç™‚è¨ºæ–·æ›¸å’Œæ”¶æ“š](https://blog.twman.org/2023/07/wsl.html)
- [TableStructureRec: è¡¨æ ¼çµæ§‹è¾¨è­˜æ¨ç†åº«ä¾†äº†](https://zhuanlan.zhihu.com/p/668484933)ï¼šhttps://github.com/RapidAI/TableStructureRec

## Document Understanding (æ–‡ä»¶ç†è§£)

[Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park, "OCR-free Document Understanding Transformer", arXiv preprint, arXiv:2111.15664, 2022.](./donut.md)

## Document Layout Analysis (æ–‡ä»¶çµæ§‹åˆ†æ)

[Zejiang Shen, Ruochen Zhang, Melissa Dell, Benjamin Charles Germain Lee, Jacob Carlson, Weining Li, "A unified toolkit for Deep Learning Based Document Image Analysis", arXiv preprint, arXiv:2103.15348, 2021.](./LayoutParser.md)

<details open>
<summary><strong>LayoutLM series</strong></summary>

- **arXiv-2020**:[Yiheng Xu,Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou,"LayoutLM: Pre-training of Text and Layout for Document Image Understanding",arXiv:1912.13318, 2020](./LayoutLM.md)
- **arXiv-2021**:[Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou,"LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding",arXiv:2012.14740, 2021](./LayoutLMv2.md)
- **arXiv-2021**:[Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Furu Wei, "LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding", arXiv:2104.08836](./LayoutXLM.md)
- **arXiv-2022**:[Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu and Furu Wei, "LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking", arXiv preprint, arXiv:2204.08387, 2022.](./LayoutLMv3.md)
</details>

[Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park,"OCR-free Document Understanding Transformer",arXiv:2111.15664,2022](./donut.md)

[Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li and Furu Wei, "TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models", arXiv preprint, arXiv:2109.10282, 2021](./TrOCR.md)

[Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang and Furu Wei, "DiT: Self-supervised Pre-training for Document Image Transformer", arXiv preprint, arXiv:2203.02378, 2022.](./DiT.md) 

## Text Recognition (æ–‡å­—è­˜åˆ¥)
Lukas Blecher, Guillem Cucurull, Thomas Scialom and Robert Stojnic, "Nougat: Neural Optical Understanding for Academic Documents", arXiv:2308.13418, 2023.
https://facebookresearch.github.io/nougat/
https://www.jiqizhixin.com/articles/2023-08-30-3

[Shancheng Fang, Hongtao Xie, Yuxin Wang, Zhendong Mao, Yongdong Zhang, "Read Like Humans: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Recognition", arXiv:2103.06495, 2021.](./ABINet.md)

[Shancheng Fang, Zhendong Mao, Hongtao Xie, Yuxin Wang, Chenggang Yan, Yongdong Zhang,"ABINet++: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Spotting",arXiv:2211.10578, 2022](./ABINet%2B%2B.md)

[Yuliang Liu, Chunhua Shen, Lianwen Jin, Tong He, Peng Chen, Chongyu Liu, Hao Chen, "ABCNet v2: Adaptive Bezier-Curve Network for Real-time End-to-end Text Spotting", arXiv preprint, 	arXiv:2105.03620, 2021.](./ABCNet_v2.md)

[Yongkun Du, Zhineng Chen, Caiyan Jia, Xiaoting Yin, Tianlun Zheng, Chenxia Li, Yuning Du, Yu-Gang Jiang, "SVTR: Scene Text Recognition with a Single Visual Model", arXiv:2205.00159,2022](./SVTR.md)

## DeepFake Detection (æ·±åº¦å½é€ åµæ¸¬)
H. Zhao, et al., "Multi-attentional Deepfake Detection", Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021, Nashville, TN, USA, 2021, pp. 2185-2194.


Sun, Zekun and Han, Yujie and Hua, Zeyu and Ruan, Na and Jia, Weijia, "Improving the Efficiency and Robustness of Deepfakes Detection through Precise Geometric Features", Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.

Xiangyu Zhu, Hao Wang, Hongyan Fei, Zhen Lei, and Stan Z. Li, "Face Forgery Detection by 3D Decomposition", Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.
