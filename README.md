<p align="center">
  <strong>Deep Learning 101, Taiwanâ€™s pioneering and highest deep learning meetup, launched on 2016/11/11 @ 83F, Taipei 101</strong>  
</p>
<p align="center">
  AIæ˜¯ä¸€æ¢å­¤ç¨ä¸”å……æ»¿æƒ¶æåŠæœªçŸ¥çš„æ—…ç¨‹ï¼ŒèŠ±ä¿çµ¢éº—çš„æ”¶è²»èª²ç¨‹æˆ–æ´»å‹•çµ•éé€šå¾€æˆåŠŸçš„æ·å¾‘ã€‚<br>
  è¡·å¿ƒæ„Ÿè¬ç•¶æ™‚ä¾†è‡ªä¸åŒå–®ä½çš„AIåŒå¥½åƒèˆ‡è€…å¯¦ååˆ†äº«çš„å¯¶è²´ç¶“é©—ï¼›å¦‚æ¬²ç§»é™¤è³‡è¨Šé‚„è«‹å‘ŠçŸ¥ã€‚<br>
  ç”± <a href="https://www.twman.org/" target="_blank">TonTon Huang Ph.D.</a> ç™¼èµ·ï¼ŒåŠå…¶ç•¶æ™‚ä»»è·å…¬å¸(å°ç£é›ªè±¹ç§‘æŠ€)ç„¡å„Ÿè´ŠåŠ©å ´åœ°åŠèŒ¶æ°´é»å¿ƒã€‚<br>
</p>  
<p align="center">
  <a href="https://huggingface.co/spaces/DeepLearning101/Deep-Learning-101-FAQ" target="_blank">
    <img src="https://github.com/Deep-Learning-101/.github/blob/main/images/DeepLearning101.JPG?raw=true" alt="Deep Learning 101" width="180"></a>
    <a href="https://www.buymeacoffee.com/DeepLearning101" target="_blank"><img src="https://cdn.buymeacoffee.com/buttons/v2/default-red.png" alt="Buy Me A Coffee" style="height: 100px !important;width: 180px !important;" ></a>
</p>
<p align="center">
  <a href="https://www.youtube.com/@DeepLearning101" target="_blank">å» YouTube è¨‚é–±</a> |
  <a href="https://www.facebook.com/groups/525579498272187/" target="_blank">Facebook</a> |
  <a href="https://deep-learning-101.github.io/"> å› GitHub Pages</a> |
  <a href="https://github.com/Deep-Learning-101" target="_blank"> åˆ° GitHub é»æ˜Ÿ</a> |  
  <a href="https://www.twman.org/DeepLearning101" target="_blank">ç¶²ç«™</a> |
  <a href="https://huggingface.co/DeepLearning101" target="_blank">åˆ° Hugging Face Space æŒ‰æ„›å¿ƒ</a>
</p>

---

<div align="center">

<table>
  <tr>
    <td align="center"><a href="https://deep-learning-101.github.io/Large-Language-Model">å¤§èªè¨€æ¨¡å‹</a></td>
    <td align="center"><a href="https://deep-learning-101.github.io/Speech-Processing">èªéŸ³è™•ç†</a></td>
    <td align="center"><a href="https://deep-learning-101.github.io/Natural-Language-Processing">è‡ªç„¶èªè¨€è™•ç†</a></td>
    <td align="center"><a href="https://deep-learning-101.github.io//Computer-Vision">é›»è…¦è¦–è¦º</a></td>
  </tr>
  <tr>
    <td><a href="https://github.com/Deep-Learning-101/Natural-Language-Processing-Paper?tab=readme-ov-file#llm">Large Language Model</a></td>
    <td><a href="https://github.com/Deep-Learning-101/Speech-Processing-Paper">Speech Processing</a></td>
    <td><a href="https://github.com/Deep-Learning-101/Natural-Language-Processing-Paper">Natural Language Processing, NLP</a></td>
    <td><a href="https://github.com/Deep-Learning-101/Computer-Vision-Paper">Computer Vision</a></td>
  </tr>
</table>

</div>

---

<details>
<summary>æ‰‹æŠŠæ‰‹å¸¶ä½ ä¸€èµ·è¸© AI å‘</summary>

<h3><a href="https://blog.twman.org/p/deeplearning101.html" target="_blank">æ‰‹æŠŠæ‰‹å¸¶ä½ ä¸€èµ·è¸© AI å‘</a>ï¼š<a href="https://www.twman.org/AI" target="_blank">https://www.twman.org/AI</a></h3>

<ul>
  <li>
    <b><a href="https://blog.twman.org/2025/03/AIAgent.html" target="_blank">é¿é–‹ AI Agent é–‹ç™¼é™·é˜±ï¼šå¸¸è¦‹å•é¡Œã€æŒ‘æˆ°èˆ‡è§£æ±ºæ–¹æ¡ˆ</a></b>ï¼š<a href="https://deep-learning-101.github.io/agent" target="_blank">æ¢è¨å¤šç¨® AI ä»£ç†äººå·¥å…·çš„æ‡‰ç”¨ç¶“é©—èˆ‡æŒ‘æˆ°ï¼Œåˆ†äº«å¯¦ç”¨ç¶“é©—èˆ‡å·¥å…·æ¨è–¦ã€‚</a>
  </li>
  <li>
    <b><a href="https://blog.twman.org/2024/08/LLM.html" target="_blank">ç™½è©±æ–‡æ‰‹æŠŠæ‰‹å¸¶ä½ ç§‘æ™® GenAI</a></b>ï¼š<a href="https://deep-learning-101.github.io/GenAI" target="_blank">æ·ºé¡¯ä»‹ç´¹ç”Ÿæˆå¼äººå·¥æ™ºæ…§æ ¸å¿ƒæ¦‚å¿µï¼Œå¼·èª¿ç¡¬é«”è³‡æºå’Œæ•¸æ“šçš„é‡è¦æ€§ã€‚</a>
  </li>
  <li>
    <b><a href="https://blog.twman.org/2024/09/LLM.html" target="_blank">å¤§å‹èªè¨€æ¨¡å‹ç›´æ¥å°±æ‰“å®Œæ”¶å·¥ï¼Ÿ</a></b>ï¼š<a href="https://deep-learning-101.github.io/1010LLM" target="_blank">å›é¡§ LLM é ˜åŸŸæ¢ç´¢æ­·ç¨‹ï¼Œè¨è«–ç¡¬é«”å‡ç´šå° AI é–‹ç™¼çš„é‡è¦æ€§ã€‚</a>
  </li>
  <li>
    <b><a href="https://blog.twman.org/2024/07/RAG.html" target="_blank">æª¢ç´¢å¢å¼·ç”Ÿæˆ(RAG)ä¸æ˜¯è¬éˆä¸¹ä¹‹å„ªåŒ–æŒ‘æˆ°æŠ€å·§</a></b>ï¼š<a href="https://deep-learning-101.github.io/RAG" target="_blank">æ¢è¨ RAG æŠ€è¡“æ‡‰ç”¨èˆ‡æŒ‘æˆ°ï¼Œæä¾›å¯¦ç”¨ç¶“é©—åˆ†äº«å’Œå·¥å…·å»ºè­°ã€‚</a>
  </li>
  <li>
    <b><a href="https://blog.twman.org/2024/02/LLM.html" target="_blank">å¤§å‹èªè¨€æ¨¡å‹ (LLM) å…¥é–€å®Œæ•´æŒ‡å—ï¼šåŸç†ã€æ‡‰ç”¨èˆ‡æœªä¾†</a></b>ï¼š<a href="https://deep-learning-101.github.io/0204LLM" target="_blank">æ¢è¨å¤šç¨® LLM å·¥å…·çš„æ‡‰ç”¨èˆ‡æŒ‘æˆ°ï¼Œå¼·èª¿ç¡¬é«”è³‡æºçš„é‡è¦æ€§ã€‚</a>
  </li>
  <li>
    <b><a href="https://blog.twman.org/2023/04/GPT.html" target="_blank">ä»€éº¼æ˜¯å¤§èªè¨€æ¨¡å‹ï¼Œå®ƒæ˜¯ä»€éº¼ï¼Ÿæƒ³è¦å—ï¼Ÿ(Large Language Modelï¼ŒLLM)</a></b>ï¼š<a href="https://deep-learning-101.github.io/GPU" target="_blank">æ¢è¨ LLM çš„ç™¼å±•èˆ‡æ‡‰ç”¨ï¼Œå¼·èª¿ç¡¬é«”è³‡æºåœ¨é–‹ç™¼ä¸­çš„é—œéµä½œç”¨ã€‚</a>
  </li>
  <li>
    <b><a href="https://blog.twman.org/2024/11/diffusion.html" target="_blank">Diffusion Model å®Œå…¨è§£æï¼šå¾åŸç†ã€æ‡‰ç”¨åˆ°å¯¦ä½œ (AI åœ–åƒç”Ÿæˆ)</a></b>ï¼›<a href="https://deep-learning-101.github.io/diffusion" target="_blank">æ·±å…¥æ¢è¨å½±åƒç”Ÿæˆèˆ‡åˆ†å‰²æŠ€è¡“çš„æ‡‰ç”¨ï¼Œå¼·èª¿ç¡¬é«”è³‡æºçš„é‡è¦æ€§ã€‚</a>
  </li>
  <li>
    <b><a href="https://blog.twman.org/2024/02/asr-tts.html" target="_blank">ASR/TTS é–‹ç™¼é¿å‘æŒ‡å—ï¼šèªéŸ³è¾¨è­˜èˆ‡åˆæˆçš„å¸¸è¦‹æŒ‘æˆ°èˆ‡å°ç­–</a></b>ï¼š<a href="https://deep-learning-101.github.io/asr-tts" target="_blank">æ¢è¨ ASR å’Œ TTS æŠ€è¡“æ‡‰ç”¨ä¸­çš„å•é¡Œï¼Œå¼·èª¿æ•¸æ“šè³ªé‡çš„é‡è¦æ€§ã€‚</a>
  </li>
  <li>
    <b><a href="https://blog.twman.org/2021/04/NLP.html" target="_blank">é‚£äº› NLP è¸©çš„å‘</a></b>ï¼š<a href="https://deep-learning-101.github.io/nlp" target="_blank">åˆ†äº« NLP é ˜åŸŸçš„å¯¦è¸ç¶“é©—ï¼Œå¼·èª¿æ•¸æ“šè³ªé‡å°æ¨¡å‹æ•ˆæœçš„å½±éŸ¿ã€‚</a>
  </li>
  <li>
    <b><a href="https://blog.twman.org/2021/04/ASR.html" target="_blank">é‚£äº›èªéŸ³è™•ç†è¸©çš„å‘</a></b>ï¼š<a href="https://deep-learning-101.github.io/speech" target="_blank">åˆ†äº«èªéŸ³è™•ç†é ˜åŸŸçš„å¯¦å‹™ç¶“é©—ï¼Œå¼·èª¿è³‡æ–™å“è³ªå°æ¨¡å‹æ•ˆæœçš„å½±éŸ¿ã€‚</a>
  </li>
  <li>
    <b><a href="https://blog.twman.org/2020/05/DeepLearning.html" target="_blank">æ‰‹æŠŠæ‰‹å­¸æ·±åº¦å­¸ç¿’å®‰è£ç’°å¢ƒ</a></b>ï¼š<a href="https://deep-learning-101.github.io/101" target="_blank">è©³ç´°ä»‹ç´¹åœ¨ Ubuntu ä¸Šå®‰è£æ·±åº¦å­¸ç¿’ç’°å¢ƒçš„æ­¥é©Ÿï¼Œåˆ†äº«å¯¦éš›æ“ä½œç¶“é©—ã€‚</a>
  </li>
</ul>

</details>

---


### **æ–‡ç« ç›®éŒ„**
- [Anomaly Detection](#anomalydetection)
- [Object Detection](#objectdetection)
- [Segmentation](#segmentation)
- [OCR](#ocr)
- [Diffusion model (æ“´æ•£æ¨¡å‹)](#diffusion-model-æ“´æ•£æ¨¡å‹)
- [Digital Human (è™›æ“¬æ•¸å­—äºº)](#digital-human-è™›æ“¬æ•¸å­—äºº)


# CV
Computer Vision (é›»è…¦è¦–è¦º)

## AnomalyDetection
**Anomaly Detectionï¼Œç•°å¸¸æª¢æ¸¬**

- 2025-07-16ï¼š[CostFilter-ADï¼šEnhancing Anomaly Detection through Matching Cost Filtering](https://github.com/ZHE-SAPI/CostFilter-AD)ï¼›[åˆ·æ–°ç„¡ç›£ç£ç•°å¸¸æª¢æ¸¬ä¸Šé™ï¼ CostFilter-ADï¼šé¦–å€‹å³æ’å³ç”¨çš„ä»£åƒ¹æ¿¾æ³¢forç•°å¸¸æª¢æ¸¬ç¯„å¼](https://zhuanlan.zhihu.com/p/1928870223529882075)
- 2025-06-13ï¼š[One-to-Normalï¼šAnomaly Personalization](https://www.alphaxiv.org/abs/2502.01201)ï¼›[å°‘æ¨£æœ¬ç•°å¸¸è¾¨è­˜æ–°çªç ´ï¼Œæ“´æ•£æ¨¡å‹å”åŠ©ç²¾æº–åµæ¸¬](https://zhuanlan.zhihu.com/p/1916799842879018831)
- 2025-06-06ï¼š[CVPR2025, *DualAnoDiff*ï¼šDual-Interrelated Diffusion Model for Few-Shot Anomaly Image Generation](https://www.alphaxiv.org/abs/2408.13509v3)ï¼›[ä»¥å¤§æ¨¡å‹æª¢æ¸¬å·¥æ¥­å“ç•°å¸¸ï¼Œå¾©æ—¦é¨°è¨Šå„ªåœ–æ–°æ¼”ç®—æ³•å…¥é¸CVPR 2025](https://www.qbitai.com/2025/06/291359.html)
- 2025-05-15ï¼š[**AdaptCLIP**: Adapting CLIP for Universal Visual Anomaly Detection](https://www.alphaxiv.org/overview/2407.15795)ï¼›[Github](https://github.com/aiiu-lab/AdaptCLIP)ï¼›[é¨°è¨Šé–‹æºAdaptCLIP æ¨¡å‹åˆ·æ–°å¤šé ˜åŸŸSOTA](https://mp.weixin.qq.com/s/w5x6T18aSZt9jxqMIdf-Yg)
- 2025-05-05ï¼š[Detect, Classify, Act: Categorizing Industrial Anomalies with Multi-Modal Large Language Models](https://www.alphaxiv.org/zh/overview/2505.02626)ï¼›[DeepWiki](https://deepwiki.com/Sassanmtr/VELM)ï¼›[æ•¸æ“šé›†](https://www.mvtec.com/company/research/datasets/mvtec-ad)
- 2025-04-27ï¼š[**AnomalyCLIP**: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection](https://www.alphaxiv.org/overview/2310.18961)ï¼›[DeepWiki](https://deepwiki.com/zqhang/AnomalyCLIP)
- 2025-04-26ï¼š[PaDim](https://www.alphaxiv.org/zh/overview/2011.08785)ï¼›[DeepWiki](https://deepwiki.com/xiahaifeng1995/PaDiM-Anomaly-Detection-Localization-master)
- 2025-04-12ï¼š[Anomaly-Aware CLIP, **AA-CLIP**: Enhancing Zero-shot Anomaly Detection via Anomaly-Aware CLIP](https://www.alphaxiv.org/zh/overview/2503.06661)ï¼›[DeepWiki](https://deepwiki.com/Mwxinnn/AA-CLIP)
- 2025-03-25ï¼š[**Dinomaly**ï¼šThe Less Is More Philosophy in Multi-Class Unsupervised Anomaly Detection](https://github.com/guojiajeremy/Dinomaly)ï¼›[ç„¡ç›£ç£ç•°å¸¸æª¢æ¸¬ï¼ˆUnsupervised Anomaly Detectionï¼ŒUADï¼‰](https://zhuanlan.zhihu.com/p/1886364053259146390)

## ObjectDetection
**Object Detection (ç›®æ¨™åµæ¸¬)**
- [AAAI2025, Multi-clue Consistency Learning to Bridge Gaps Between General and Oriented Object in Semi-supervised Detection](https://www.alphaxiv.org/abs/2407.05909)ï¼›[Github](https://github.com/facias914/sood-mcl)ï¼›[AAAI2025 ä¸€å€‹é™æ„ŸåŠç›£ç£ç›®æ¨™åµæ¸¬ï¼ˆåŠç›£ç£æ—‹è½‰ç›®æ¨™åµæ¸¬ï¼‰æ–¹æ³•](https://zhuanlan.zhihu.com/p/26788012528)
- 2025-07-24ï¼š[OV-DINO](https://github.com/wanghao9610/OV-DINO)ï¼›[é–‹æºå·¥æ¥­é–‹æ”¾è©å½™ç›®æ¨™åµæ¸¬](https://mp.weixin.qq.com/s/gLAVYFAH_39gT4XC0zWN0A)
- 2025-06-18ï¼š[CountVid: Open-World Object Counting in Videos](https://www.alphaxiv.org/abs/2506.15368)ï¼›[ç‰›æ´¥å¤§å­¸é–‹æºé¡åˆ¥ç„¡é—œçš„å½±ç‰‡ç›®æ¨™è¨ˆæ•¸ï¼Œå½±ç‰‡ä¸­ä¹Ÿèƒ½ã€ŒæŒ‡å“ªæ•¸å“ªã€](https://mp.weixin.qq.com/s/hICrrfEgriyktoIxnbjPEQ)
- 2025-06-15ï¼š[GeoPix](https://github.com/Norman-Ou/GeoPix)ï¼›[åƒç´ ç´šé™æ„Ÿå¤šæ¨¡æ…‹å¤§æ¨¡å‹](https://3slab.pku.edu.cn/info/1026/2121.htm)
- 2025-05-23ï¼š[VisionReasoner](https://github.com/dvlab-research/VisionReasoner)ï¼›[åµæ¸¬ã€åˆ†å‰²ã€è¨ˆæ•¸ã€å•ç­”å…¨æ‹¿ä¸‹ï¼Ÿå°æ¨™Qwen2.5-VLï¼ VisionReasonerç”¨å¼·åŒ–å­¸ç¿’çµ±ä¸€è¦–è¦ºæ„ŸçŸ¥èˆ‡æ¨ç†](https://mp.weixin.qq.com/s/vECz3i_-dzvlDr3BdRLPWQ)
- 2025-03-14ï¼š[Falcon: A Remote Sensing Vision-Language Foundation Model](https://www.alphaxiv.org/abs/2503.11070)ï¼›[DeepWiki](https://deepwiki.com/TianHuiLab/Falcon)
 

## Segmentation
**Segmentation (åœ–åƒåˆ†å‰²)**
- [Perceive Anything Modelï¼šRecognize, Explain, Caption, and Segment Anything in Images and Videos](https://www.alphaxiv.org/zh/overview/2506.05302v1)ï¼›[å°æ¨™SAM2 + LLMèåˆç‰ˆï¼æ¸¯ä¸­æ–‡é–‹æºæ„ŸçŸ¥ä¸€åˆ‡æ¨¡å‹èˆ‡ç™¾è¬ç´šå½±åƒæè¿°è³‡æ–™é›†ï¼šè¾¨è­˜ã€è§£é‡‹ã€æè¿°ã€åˆ†å‰²ä¸€é«”åŒ–è¼¸å‡º](https://zhuanlan.zhihu.com/p/1919709726209446971)
- [RemoteSAM](https://www.alphaxiv.org/abs/2505.18022v3)ï¼š[Towards Segment Anything for Earth Observation](https://deepwiki.com/1e12Leon/RemoteSAM)
- [InstructSAM](https://voyagerxvoyagerx.github.io/InstructSAM/)ï¼š[A Training-Free Framework for Instruction-Oriented Remote Sensing Object Recognition](https://www.alphaxiv.org/zh/overview/2505.15818v1)ï¼›[DeepWiki](https://deepwiki.com/VoyagerXvoyagerx/InstructSAM)
- [RESAnything: Attribute Prompting for Arbitrary Referring Segmentation](https://www.alphaxiv.org/abs/2505.02867)ï¼›[Project](https://suikei-wang.github.io/RESAnything/)
- [CVPR 2025, Segment Any Motion in Videos, Segment Any Motion in Videos](https://www.alphaxiv.org/zh/overview/2503.22268)ï¼›[Github](https://github.com/nnanhuang/SegAnyMo)
- [CVPR 2025 Highlight, Exact: Exploring Space-Time Perceptive Clues for Weakly Supervised Satellite Image Time Series Semantic Segmentation](https://www.alphaxiv.org/zh/overview/2412.03968)ï¼›[Github](https://github.com/MiSsU-HH/Exact)ï¼›[Exactï¼šåŸºæ–¼é™æ„Ÿå½±åƒæ™‚é–“åºåˆ—å¼±ç›£ç£å­¸ç¿’çš„ä½œç‰©æå–æ–¹æ³•](https://zhuanlan.zhihu.com/p/38754229963)
- [MatAnyone](https://github.com/pq-yang/MatAnyone)ï¼š[è¦–è¨Šæ‘³åœ–MatAnyoneä¾†äº†ï¼Œä¸€æ¬¡æŒ‡å®šå…¨ç¨‹è¿½è¸ªï¼Œé«®çµ²ç´šé‚„åŸ](https://www.jiqizhixin.com/articles/2025-04-17-27)
- [Meta Segment Anything Model 2 (SAM 2)](https://ai.meta.com/sam2/)
   - [60è¡Œç¨‹å¼ç¢¼è¨“ç·´/å¾®èª¿Segment Anything 2](https://mp.weixin.qq.com/s/YfgYCzvi0cXxOFIfQvE_9w)
   - [CLIPSegï¼šImage Segmentation Using Text and Image Prompts](https://github.com/timojl/clipseg)ï¼š[Huggingface Space](https://huggingface.co/spaces/taesiri/CLIPSeg)
      - [å“¥å»·æ ¹å¤§å­¸æå‡ºCLIPSegï¼Œèƒ½åŒæ™‚ä½œä¸‰å€‹åˆ†å‰²ä»»å‹™çš„æ¨¡å‹](https://mp.weixin.qq.com/s/evKssKulZiUssLN71t6_Lw)
      - [SAMèˆ‡CLIPå¼·å¼·è¯æ‰‹ï¼Œå¯¦ç¾22000é¡çš„åˆ†å‰²èˆ‡è­˜åˆ¥](https://mp.weixin.qq.com/s/evKssKulZiUssLN71t6_Lw)
- [SAMURAI](https://yangchris11.github.io/samurai/)
   - [ç„¡éœ€è¨“ç·´æˆ–å¾®èª¿å³å¯å¾—åˆ°ç©©å®šã€æº–ç¢ºçš„è¿½è¹¤æ•ˆæœï¼ KF + SAM2 è§£æ±ºå¿«é€Ÿç§»å‹•æˆ–è‡ªé®æ“‹çš„ç‰©ä»¶è¿½è¹¤å•é¡Œ](https://mp.weixin.qq.com/s/iU3Bk_uO01GWUxAtIBsrWQ)
   - [ç¶“å…¸å¡çˆ¾æ›¼æ¿¾æ³¢å™¨æ”¹é€²å½±ç‰‡ç‰ˆã€Œåˆ†å‰²ä¸€åˆ‡ã€ï¼Œç¶²å‹ï¼šå¥½å„ªé›…çš„æ–¹æ³•](https://www.qbitai.com/2024/11/223020.html)
- [Grounded SAM 2: Ground and Track Anything in Videos](https://github.com/IDEA-Research/Grounded-SAM-2)
   - [Grounded-Segment-Anything](https://huggingface.co/spaces/yizhangliu/Grounded-Segment-Anything)
- [SAM2Long](https://github.com/Mark12Ding/SAM2Long)ï¼š[å¤§å¹…æå‡SAM 2æ€§èƒ½ï¼æ¸¯ä¸­æ–‡æå‡ºSAM2Longï¼Œè¤‡é›œé•·è¦–é »çš„åˆ†å‰²æ¨¡å‹](https://mp.weixin.qq.com/s/henvaxGoNgx24NLQV1Qj2w)
- [SAM2-Adapter](https://github.com/tianrun-chen/SAM-Adapter-PyTorch)ï¼š[SAM 2ç„¡æ³•åˆ†å‰²ä¸€åˆ‡ï¼Ÿ SAM2-Adapterï¼šé¦–æ¬¡è®“SAM 2åœ¨ä¸‹æ¸¸ä»»å‹™é©æ‡‰èª¿æ ¡ï¼](https://mp.weixin.qq.com/s/3z-LshKAgbSzNCzyoLOuag)
- [SAM2Point](https://github.com/ZiyuGuo99/SAM2Point)ï¼š[å¯æç¤º3D åˆ†å‰²ç ”ç©¶é‡Œç¨‹ç¢‘ï¼ SAM2Pointï¼šSAM2åŠ æŒå¯æ³›åŒ–ä»»3Då ´æ™¯ã€ä»»æ„æç¤ºï¼](https://mp.weixin.qq.com/s/TnTK5UE7O_hcrNzloxBmAw)

- [Optical Character Recognitionï¼Œå…‰å­¸æ–‡å­—è­˜åˆ¥](#OCR)

## OCR
**Optical Character Recognition (å…‰å­¸æ–‡å­—è­˜åˆ¥)**  
**[é‡å°ç‰©ä»¶æˆ–å ´æ™¯å½±åƒé€²è¡Œåˆ†æèˆ‡åµæ¸¬](https://www.twman.org/AI/CV)**
- [2025-08-18]ï¼š[DianJin-OCR-R1](https://github.com/aliyun/qwen-dianjin)ï¼›[é»é‡‘OCR-R1ï¼Œæ¨¡ç³Šè“‹ç« ã€è·¨é è¡¨æ ¼ã€æ–‡å­—å¹»è¦ºå…¨æ‹¿ä¸‹ï¼](https://mp.weixin.qq.com/s/cOo0sqwDt3ARid70wBaYVA)
- 2025-07-30ï¼š[dots.ocr](https://huggingface.co/rednote-hilab/dots.ocr)ï¼›[æœ¬åœ°éƒ¨ç½²1.7Bå‚æ•°è¶…å¼ºOCRå¤§æ¨¡å‹dots.ocr](https://zhuanlan.zhihu.com/p/1935120171573413613)
- 2025-06-16ï¼š[OCRFlux](https://github.com/chatdoc-com/OCRFlux)ï¼›[DEMO](https://ocrflux.pdfparser.io/#/)ï¼›[OCRFluxï¼šä¸€å€‹åŸºæ–¼LLMçš„è¤‡é›œä½ˆå±€èˆ‡è·¨é åˆä½µçš„PDFæ–‡æª”è§£æ](https://mp.weixin.qq.com/s/UgkKLYsVxrrFy-5Gn0JD-g)
- 2025-06-05ï¼š[MonkeyOCR](https://deepwiki.com/Yuliang-Liu/MonkeyOCR)ï¼›[Document Parsing with a Structure-Recognition-Relation Triplet Paradigm](https://www.alphaxiv.org/overview/2506.05218)
- 2025-05-21ï¼š[PaddleOCR 3.0](https://github.com/PaddlePaddle/PaddleOCR/tree/release/3.0)ï¼›[OCRç²¾æº–åº¦èºå‡13%ï¼Œæ”¯æ´å¤šèªç¨®ã€æ‰‹å¯«é«”èˆ‡é«˜ç²¾æº–åº¦æ–‡ä»¶è§£æ](https://zhuanlan.zhihu.com/p/1908447391784342470)
- 2025-03-05ï¼š[PP-DocBee](https://github.com/PaddlePaddle/PaddleMIX/tree/develop/deploy/ppdocbee)ï¼š[ç™¾åº¦æ¨å‡ºæ–‡ä»¶å½±åƒç†è§£PP-DocBee](https://zhuanlan.zhihu.com/p/28715553656)
- 2025-03-03ï¼š[olmocr](https://github.com/allenai/olmocr)ï¼š[ğŸš€æœ¬åœ°éƒ¨ç½²æœ€å¼ºOCRå¤§æ¨¡å‹olmOCRï¼æ”¯æŒç»“æ„åŒ–ç²¾å‡†æå–å¤æ‚PDFæ–‡ä»¶å†…å®¹ï¼](https://www.aivi.fyi/llms/deploy-olmOCR)
- 2025-02-05ï¼š[MinerU](https://github.com/opendatalab/MinerU)ï¼š[å°‡PDFè½‰æ›ç‚ºæ©Ÿå™¨å¯è®€æ ¼å¼çš„ç¥å™¨](https://mp.weixin.qq.com/s/ci5wp6gICTCtaRZfn5yWUQ)
- 2024-12-15ï¼š[markitdown](https://github.com/microsoft/markitdown)
- 2024-09-22ï¼š[OCR2.0æ—¶ä»£-GOTæ¥å•¦ï¼](https://mp.weixin.qq.com/s/W-Ult-F3pU6Wvx3fHEN8yA)
- 2024-09-11ï¼š[GOT-OCR-2.0æ¨¡å‹å¼€æº](https://mp.weixin.qq.com/s/rQL-Q0TGhT6e8Ti4zZalrg)
- 2024-08-20ï¼š[è¬ç‰©çš†å¯AIåŒ–ï¼å‰›é–‹æºå°±æœ‰12000äººåœè§€çš„OCR æƒæPDF é–‹æºå·¥å…·ï¼é‚„å¯è½‰æ›ç‚ºMarkDownï¼](https://www.53ai.com/news/MultimodalLargeModel/2024082059736.html)
- [advancedliteratemachinery/OCR/OmniParser](https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/OmniParser)
- 2024-10-29ï¼š[Alibabaå‡ºå“:OmniParseré€šç”¨æ–‡æª”è¤‡é›œå ´æ™¯ä¸‹OCRæŠ½å–](https://mp.weixin.qq.com/s/_1Aatpna7poIVRhfYk4aAQ)
- [RapidOCR](https://github.com/RapidAI/RapidOCR/blob/main/docs/README_zh.md)
- [12å€‹æµè¡Œçš„é–‹æºå…è²»OCRé …ç›®](https://mp.weixin.qq.com/s/7EuhnQedAX6injBL_Dg_sQ)
- [ç”¨PaddleOCRçš„PPOCRLabelä¾†å¾®èª¿é†«ç™‚è¨ºæ–·æ›¸å’Œæ”¶æ“š](https://blog.twman.org/2023/07/wsl.html)
- [TableStructureRec: è¡¨æ ¼çµæ§‹è¾¨è­˜æ¨ç†åº«ä¾†äº†](https://zhuanlan.zhihu.com/p/668484933)ï¼šhttps://github.com/RapidAI/TableStructureRec

## Diffusion model (æ“´æ•£æ¨¡å‹)
- 2025-05-28ï¼š[è¦–è¦ºç†è§£&ç”Ÿæˆå¤§ä¸€çµ±æ¨¡å‹ Jodi](https://vipl-genun.github.io/Project-Jodi/)ï¼›[alphaXiv](https://www.alphaxiv.org/zh/overview/2505.19084)
- 2025-05-27ï¼š[AnomalyAny](https://hansunhayden.github.io/AnomalyAny.github.io/)ï¼›[CVPR2025ï½œçªç ´è³‡æ–™ç“¶é ¸ï¼ Stable Diffusion å”åŠ©è¦–è¦ºç•°å¸¸æª¢æ¸¬ï¼Œç„¡éœ€è¨“ç·´å³å¯ç”¢ç”ŸçœŸå¯¦å¤šæ¨£ç•°å¸¸æ¨£æœ¬](https://zhuanlan.zhihu.com/p/1910284073231942689)
- 2025-05-23ï¼š[HivisionIDPhotosï¼Œæ™ºæ…§è­‰ä»¶ç…§ç”¢ç”Ÿç¥å™¨](https://deepwiki.com/Zeyi-Lin/HivisionIDPhotos)ï¼›[AIè­‰ä»¶ç…§ï¼Œæ‘³åœ–ã€æ›èƒŒæ™¯ã€ä»»æ„å°ºå¯¸](https://zhuanlan.zhihu.com/p/718725351)
- 2025-05-19ï¼š[Index-AniSora](https://deepwiki.com/bilibili/Index-anisora)ï¼›[Aligning Anime Video Generation with Human Feedback](https://www.alphaxiv.org/overview/2504.10044)ï¼›[Bç«™é–‹æºSOTAå‹•ç•«å½±ç‰‡ç”Ÿæˆæ¨¡å‹Index-AniSoraï¼](https://zhuanlan.zhihu.com/p/1908150671540224717)
- 2025-04-26ï¼š[Insert Anything](https://www.alphaxiv.org/zh/overview/2504.15009)ï¼›[DeepWiki](https://deepwiki.com/song-wensong/insert-anything)  
- 2025-04-24ï¼š[å­—ç¯€Phantom](https://github.com/Phantom-video/Phantom)ï¼š[1280x720å½±ç‰‡ç”Ÿæˆé©å‘½ï¼ä½å…ƒçµ„Phantomæ¨¡å‹å¯¦æ¸¬ï¼š10Gé¡¯å­˜æ•ˆæœä¸è¼¸æŸéˆä»˜è²»ç‰ˆ](https://zhuanlan.zhihu.com/p/1898688574477545694)
- 2025-04-22ï¼š[MAGI-1](https://github.com/SandAI-org/Magi-1)ï¼š[Sand AI å‰µæ¥­åœ˜éšŠæ¨å‡ºäº†å…¨çƒé¦–å€‹è‡ªå›æ­¸å½±ç‰‡ç”Ÿæˆå¤§æ¨¡å‹MAGI-1ï¼Œè©²æ¨¡å‹æœ‰å“ªäº›æ•ˆèƒ½äº®é»ï¼Ÿ](https://www.zhihu.com/question/1898030232184795448)
- 2025-04-22ï¼š[SkyReels V2](https://github.com/SkyworkAI/SkyReels-V2)ï¼š[å…¨çƒé¦–å€‹ç„¡é™æ™‚é•·å½±ç‰‡ç”Ÿæˆï¼æ–°æ“´æ•£æ¨¡å¼å¼•çˆ†å…†å¸‚å ´ï¼Œé›»å½±ç´šç†è§£ï¼Œå…¨é¢é–‹æº](https://www.qbitai.com/2025/04/275531.html)
- 2025-04-14ï¼š[FramePack](https://github.com/kijai/ComfyUI-FramePackWrapper)ï¼š[ä¸æ˜¯å¯éˆç”¨ä¸èµ·ï¼Œè€Œæ˜¯FramePackæ›´æœ‰æ€§åƒ¹æ¯”ï¼é–‹æºå°ˆæ¡ˆï¼š6Gé¡¯å­˜è·‘13Bæ¨¡å‹ï¼Œæ”¯æ´1åˆ†é˜å½±ç‰‡ç”¢ç”Ÿ](https://zhuanlan.zhihu.com/p/1896487969470251546)
- 2025-04-14ï¼š[fantasy-talking](https://fantasy-amap.github.io/fantasy-talking/)ï¼š[è§£è®€æœ€æ–°åŸºæ–¼Wan2.1çš„éŸ³è¨Šé©…å‹•æ•¸ä½äººFantasyTalking](https://zhuanlan.zhihu.com/p/1892895916354148118)
- 2025-04-05ï¼š[SkyReels-A2](https://www.alphaxiv.org/zh/overview/2504.02436)ï¼›[DeepWiki](https://deepwiki.com/SkyworkAI/SkyReels-A2)ï¼›[SkyReels-A2ï¼šç”¨AIé‡æ–°å®šä¹‰è§†é¢‘åˆ›ä½œçš„æœªæ¥ï¼](https://zhuanlan.zhihu.com/p/1892709305301590652)
- 2025-03-10ï¼š[HunyuanVideo-I2V](https://github.com/Tencent/HunyuanVideo-I2V)ï¼š[é¨°è¨Šé–‹æºHunyuanVideo-I2Våœ–ç”Ÿè¦–è¨Šæ¨¡å‹+LoRAè¨“ç·´è…³æœ¬ï¼Œç¤¾ç¾¤éƒ¨ç½²ã€æ¨ç†å¯¦æˆ°æ•™å­¸ä¾†å§](https://zhuanlan.zhihu.com/p/29110060025)
- 2025-02-25ï¼š[Wan-Video](https://github.com/Wan-Video/Wan2.1)ï¼š[è¶…è¶ŠSoraï¼é˜¿é‡Œè¬ç›¸å¤§æ¨¡å‹æ­£å¼é–‹æºï¼å…¨æ¨¡æ…‹ã€å…¨å°ºå¯¸å¤§æ¨¡å‹é–‹æº](https://finance.sina.com.cn/jjxw/2025-02-26/doc-inemukxr9127437.shtml)
- 2025-02-14ï¼š[FlashVideo](https://github.com/FoundationVision/FlashVideo)ï¼š[ä¾†è‡ªä½å…ƒçµ„çš„è¦–è¨Šå¢å¼·å…¨æ–°é–‹æºæ¼”ç®—æ³•ï¼Œ102ç§’ç”¢ç”Ÿ1080Pè¦–é »](https://zhuanlan.zhihu.com/p/23702953115)
- 2025-01-28ï¼š[Sana](https://github.com/NVlabs/Sana)ï¼š[ICLR 2025 Oral] Efficient High-Resolution Image Synthesis with Linear Diffusion Transformerï¼›[æ¯”FLUXå¿«100å€ï¼è‹±å‰é”è¯æ‰‹MITã€æ¸…è¯é–‹æºè¶…å¿«AIå½±åƒç”¢ç”Ÿæ¨¡å‹](https://zhuanlan.zhihu.com/p/19489214543)
- [Flux](https://huggingface.co/black-forest-labs)
   - [Flux.1-canny-dev](https://huggingface.co/spaces/black-forest-labs/FLUX.1-canny-dev)ï¼š[https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev/](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev/)
   - [Flux.1-depth-dev](https://huggingface.co/spaces/black-forest-labs/FLUX.1-Depth-dev)ï¼š[https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev/](https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev/)
   - [Flux.1-fill-dev](https://huggingface.co/spaces/black-forest-labs/FLUX.1-Fill-dev)ï¼š[https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev/](https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev/)
   - [Flux.1-redux-dev](https://huggingface.co/spaces/black-forest-labs/FLUX.1-Redux-dev)ï¼š[https://huggingface.co/black-forest-labs/FLUX.1-Redux-dev/](https://huggingface.co/black-forest-labs/FLUX.1-Redux-dev/)
      - 2024-11-26ï¼š[Fluxå®˜æ–¹é‡ç¹ª+æ“´åœ–+é¢¨æ ¼åƒè€ƒ+ControlNet](https://mp.weixin.qq.com/s/Kj1nyJNTpoZ94JjO4FMw_g)
      - 2024-11-25ï¼š[æœ€æ–°flux_fill_inpaintæ¨¡å‹é«”é©—ã€‚](https://mp.weixin.qq.com/s/OPknDJXH1_oezSR86c_png)
- 2024-12-17ï¼š[Leffa](https://github.com/franciszzj/Leffa)ï¼š[Leffaï¼šMeta AI é–‹æºç²¾ç¢ºæ§åˆ¶äººç‰©å¤–è§€å’Œå§¿å‹¢çš„åœ–åƒç”Ÿæˆæ¡†æ¶ï¼Œåœ¨ç”Ÿæˆç©¿è‘—çš„åŒæ™‚ä¿æŒäººç‰©ç‰¹å¾µ](https://juejin.cn/post/7449325873725276196)
- 2024-11-29ï¼š[PuLID, Pure and Lightning ID Customization via Contrastive Alignment](https://github.com/ToTheBeginning/PuLID)ï¼š[https://github.com/balazik/ComfyUI-PuLID-Flux](https://github.com/balazik/ComfyUI-PuLID-Flux)
   - 2024-11-07ï¼š[æå®šComfyUI-PuLID-Fluxç¯€é»åªè¦é€™å¹¾æ­¥ï¼é™„ä¸€éµå£“ç¸®åŒ…](https://mp.weixin.qq.com/s/07BMFHaSasl7-PFtkN6_Zg)
   - 2024-10-08ï¼š[ä¸€æ–‡ææ‡‚PuLID FLUXäººç‰©æ›è‡‰&é¢¨æ ¼é·ç§»](https://mp.weixin.qq.com/s/V-2Cp8_xFnHQNFn35aGdLg)
- 2024-11-26ï¼š[MagicQuill](https://github.com/magic-quill/MagicQuill)ï¼š[https://huggingface.co/spaces/AI4Editing/MagicQuill](https://huggingface.co/spaces/AI4Editing/MagicQuill)
   - [MagicQuillï¼Œç™»ä¸ŠHuggingfaceè¶¨å‹¢æ¦œæ¦œé¦–çš„AI Påœ–ç¥å™¨](https://mp.weixin.qq.com/s/Pc3xRP8_9BxkVSRNznkplw)
- 2024-11-26ï¼š[OOTDiffusion](https://github.com/levihsu/OOTDiffusion)ï¼š[https://huggingface.co/spaces/levihsu/OOTDiffusion](https://huggingface.co/spaces/levihsu/OOTDiffusion)
   - [é–‹æºAIæ›è£ç¥å™¨OOTDiffusion](https://mp.weixin.qq.com/s/B2rNCjJLo8coYzoHGPnVaw)
- 2024-11-24ï¼š[Comfyui Impact Pack](https://github.com/ltdrdata/ComfyUI-Impact-Pack)
   - [Comfyui æœ€å¼·è‡‰éƒ¨ä¿®å¾©å·¥å…·Impact Pack](https://mp.weixin.qq.com/s/hNQ9BfdGbRQ_Osus-yMJWg)
- 2024-11-05ï¼š[ComfyUI OmniGen @ åŒ—äº¬äººå·¥æ™ºæ…§ç ”ç©¶é™¢](https://github.com/AIFSH/OmniGen-ComfyUI)ï¼š[https://huggingface.co/spaces/Shitao/OmniGen](https://huggingface.co/spaces/Shitao/OmniGen)
   - [ComfyUI å½±åƒç”Ÿæˆæ¨¡å‹OmniGenï¼Œäººç‰©ä¸€è‡´æ€§è™•ç†çš„ä¹Ÿå¤ªå¥½äº†](https://mp.weixin.qq.com/s/msGK0FmNs3T3jbUBHfR9DA)
   - [å…¨èƒ½å½±åƒç”Ÿæˆæ¨¡å‹OmniGenï¼šå‘Šåˆ¥ControlNetã€ipadapterç­‰æ’ä»¶ï¼Œåƒ…æ†‘æç¤ºå³å¯æ§åˆ¶å½±åƒç”Ÿæˆèˆ‡ç·¨è¼¯](https://mp.weixin.qq.com/s/48HmqRGBOK1uBdzlprdKSA)


## Digital Human (è™›æ“¬æ•¸å­—äºº)
- [HeyGem](https://github.com/GuijiAI/HeyGem.ai)ï¼š[é–‹æºæ•¸ä½äººå…‹éš†ç¥å™¨](https://zhuanlan.zhihu.com/p/29274862393)
- [Duix](https://github.com/GuijiAI/duix.ai)ï¼š[å…¨çƒé¦–å€‹çœŸäººæ•¸ä½äººï¼Œé–‹æºäº†](https://zhuanlan.zhihu.com/p/716583514)
- [Linly-Talker](https://github.com/Kedreamix/Linly-Talker)ï¼šan intelligent AI system that combines large language models (LLMs) with visual models to create a novel human-AI interaction method. 
- [EchoMimicV2](https://github.com/antgroup/echomimic_v2)ï¼š[CVPR 2025] EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation
- [Hallo3](https://github.com/fudan-generative-vision/hallo3)ï¼š[CVPR 2025] Highly Dynamic and Realistic Portrait Image Animation with Diffusion Transformer Networks
- [MimicTalk](https://github.com/yerfor/MimicTalk)ï¼š[NeurIPS 2024] MimicTalk: Mimicking a personalized and expressive 3D talking face in minutes
- [JoyGen](https://github.com/JOY-MM/JoyGen)ï¼šAudio-Driven 3D Depth-Aware Talking-Face Video Editing
- [Latentsync](https://github.com/bytedance/LatentSync)
- [MuseTalk](https://github.com/TMElyralab/MuseTalk)



## Image Recognition (åœ–åƒè­˜åˆ¥)

- [ViTï¼ˆVision Transformerï¼‰è§£æ](https://zhuanlan.zhihu.com/p/445122996)ï¼šhttps://github.com/google-research/vision_transformer
- [2040å¼µåœ–ç‰‡è¨“ç·´å‡ºçš„ViTï¼Œæº–ç¢ºç‡96.7%ï¼Œé€£é·ç§»è¡¨ç¾éƒ½ä»¤äººé©šè¨](https://zhuanlan.zhihu.com/p/463608959)
- [Swin Transformer: ç”¨CNNçš„æ–¹å¼æ‰“æ•—CNN](https://zhuanlan.zhihu.com/p/362690149)ï¼šhttps://github.com/microsoft/Swin-Transformer
- [EfficientNetV2éœ‡æ’¼ç™¼å¸ƒï¼æ›´å°çš„æ¨¡å‹ï¼Œæ›´å¿«çš„è¨“ç·´](https://zhuanlan.zhihu.com/p/361873583)ï¼šhttps://github.com/d-li14/efficientnetv2.pytorch

## Document Understanding (æ–‡ä»¶ç†è§£)

[Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park, "OCR-free Document Understanding Transformer", arXiv preprint, arXiv:2111.15664, 2022.](./donut.md)

## Document Layout Analysis (æ–‡ä»¶çµæ§‹åˆ†æ)

[Zejiang Shen, Ruochen Zhang, Melissa Dell, Benjamin Charles Germain Lee, Jacob Carlson, Weining Li, "A unified toolkit for Deep Learning Based Document Image Analysis", arXiv preprint, arXiv:2103.15348, 2021.](./LayoutParser.md)

<details open>
<summary><strong>LayoutLM series</strong></summary>

- **arXiv-2020**:[Yiheng Xu,Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou,"LayoutLM: Pre-training of Text and Layout for Document Image Understanding",arXiv:1912.13318, 2020](./LayoutLM.md)
- **arXiv-2021**:[Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou,"LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding",arXiv:2012.14740, 2021](./LayoutLMv2.md)
- **arXiv-2021**:[Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Furu Wei, "LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding", arXiv:2104.08836](./LayoutXLM.md)
- **arXiv-2022**:[Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu and Furu Wei, "LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking", arXiv preprint, arXiv:2204.08387, 2022.](./LayoutLMv3.md)
</details>

[Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park,"OCR-free Document Understanding Transformer",arXiv:2111.15664,2022](./donut.md)

[Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li and Furu Wei, "TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models", arXiv preprint, arXiv:2109.10282, 2021](./TrOCR.md)

[Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang and Furu Wei, "DiT: Self-supervised Pre-training for Document Image Transformer", arXiv preprint, arXiv:2203.02378, 2022.](./DiT.md) 

## Text Recognition (æ–‡å­—è­˜åˆ¥)
Lukas Blecher, Guillem Cucurull, Thomas Scialom and Robert Stojnic, "Nougat: Neural Optical Understanding for Academic Documents", arXiv:2308.13418, 2023.
https://facebookresearch.github.io/nougat/
https://www.jiqizhixin.com/articles/2023-08-30-3

[Shancheng Fang, Hongtao Xie, Yuxin Wang, Zhendong Mao, Yongdong Zhang, "Read Like Humans: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Recognition", arXiv:2103.06495, 2021.](./ABINet.md)

[Shancheng Fang, Zhendong Mao, Hongtao Xie, Yuxin Wang, Chenggang Yan, Yongdong Zhang,"ABINet++: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Spotting",arXiv:2211.10578, 2022](./ABINet%2B%2B.md)

[Yuliang Liu, Chunhua Shen, Lianwen Jin, Tong He, Peng Chen, Chongyu Liu, Hao Chen, "ABCNet v2: Adaptive Bezier-Curve Network for Real-time End-to-end Text Spotting", arXiv preprint, 	arXiv:2105.03620, 2021.](./ABCNet_v2.md)

[Yongkun Du, Zhineng Chen, Caiyan Jia, Xiaoting Yin, Tianlun Zheng, Chenxia Li, Yuning Du, Yu-Gang Jiang, "SVTR: Scene Text Recognition with a Single Visual Model", arXiv:2205.00159,2022](./SVTR.md)

## DeepFake Detection (æ·±åº¦å½é€ åµæ¸¬)
H. Zhao, et al., "Multi-attentional Deepfake Detection", Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021, Nashville, TN, USA, 2021, pp. 2185-2194.


Sun, Zekun and Han, Yujie and Hua, Zeyu and Ruan, Na and Jia, Weijia, "Improving the Efficiency and Robustness of Deepfakes Detection through Precise Geometric Features", Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.

Xiangyu Zhu, Hao Wang, Hongyan Fei, Zhen Lei, and Stan Z. Li, "Face Forgery Detection by 3D Decomposition", Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.
