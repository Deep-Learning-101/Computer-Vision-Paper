<p align="center">
  <strong>Deep Learning 101, Taiwanâ€™s pioneering and highest deep learning meetup, launched on 2016/11/11 @ 83F, Taipei 101</strong>  
</p>
<p align="center">
  AIæ˜¯ä¸€æ¢å­¤ç¨ä¸”å……æ»¿æƒ¶æåŠæœªçŸ¥çš„æ—…ç¨‹ï¼ŒèŠ±ä¿çµ¢éº—çš„æ”¶è²»èª²ç¨‹æˆ–æ´»å‹•çµ•éé€šå¾€æˆåŠŸçš„æ·å¾‘ã€‚<br>
  è¡·å¿ƒæ„Ÿè¬ç•¶æ™‚ä¾†è‡ªä¸åŒå–®ä½çš„AIåŒå¥½åƒèˆ‡è€…å¯¦ååˆ†äº«çš„å¯¶è²´ç¶“é©—ï¼›å¦‚æ¬²ç§»é™¤è³‡è¨Šé‚„è«‹å‘ŠçŸ¥ã€‚<br>
  ç”± <a href="https://www.twman.org/" target="_blank">TonTon Huang Ph.D.</a> ç™¼èµ·ï¼ŒåŠå…¶ç•¶æ™‚ä»»è·å…¬å¸(å°ç£é›ªè±¹ç§‘æŠ€)ç„¡å„Ÿè´ŠåŠ©å ´åœ°åŠèŒ¶æ°´é»å¿ƒã€‚<br>
</p>  
<p align="center">
  <a href="https://huggingface.co/spaces/DeepLearning101/Deep-Learning-101-FAQ" target="_blank">
    <img src="https://github.com/Deep-Learning-101/.github/blob/main/images/DeepLearning101.JPG?raw=true" alt="Deep Learning 101" width="180"></a>
    <a href="https://www.buymeacoffee.com/DeepLearning101" target="_blank"><img src="https://cdn.buymeacoffee.com/buttons/v2/default-red.png" alt="Buy Me A Coffee" style="height: 100px !important;width: 180px !important;" ></a>
</p>
<p align="center">
  <a href="https://www.youtube.com/@DeepLearning101" target="_blank">å» YouTube è¨‚é–±</a> |
  <a href="https://www.facebook.com/groups/525579498272187/" target="_blank">Facebook</a> |
  <a href="https://deep-learning-101.github.io/"> å› GitHub Pages</a> |
  <a href="https://github.com/Deep-Learning-101" target="_blank"> åˆ° GitHub é»æ˜Ÿ</a> |  
  <a href="https://www.twman.org/DeepLearning101" target="_blank">ç¶²ç«™</a> |
  <a href="https://huggingface.co/DeepLearning101" target="_blank">åˆ° Hugging Face Space æŒ‰æ„›å¿ƒ</a>
</p>

---

<div align="center">

<table>
  <tr>
    <td align="center"><a href="https://deep-learning-101.github.io/Large-Language-Model">å¤§èªè¨€æ¨¡å‹</a></td>
    <td align="center"><a href="https://deep-learning-101.github.io/Speech-Processing">èªéŸ³è™•ç†</a></td>
    <td align="center"><a href="https://deep-learning-101.github.io/Natural-Language-Processing">è‡ªç„¶èªè¨€è™•ç†</a></td>
    <td align="center"><a href="https://deep-learning-101.github.io//Computer-Vision">é›»è…¦è¦–è¦º</a></td>
  </tr>
  <tr>
    <td><a href="https://github.com/Deep-Learning-101/Natural-Language-Processing-Paper?tab=readme-ov-file#llm">Large Language Model</a></td>
    <td><a href="https://github.com/Deep-Learning-101/Speech-Processing-Paper">Speech Processing</a></td>
    <td><a href="https://github.com/Deep-Learning-101/Natural-Language-Processing-Paper">Natural Language Processing, NLP</a></td>
    <td><a href="https://github.com/Deep-Learning-101/Computer-Vision-Paper">Computer Vision</a></td>
  </tr>
</table>

</div>

---

<details>
<summary>æ‰‹æŠŠæ‰‹å¸¶ä½ ä¸€èµ·è¸© AI å‘</summary>

<h3><a href="https://blog.twman.org/p/deeplearning101.html" target="_blank">æ‰‹æŠŠæ‰‹å¸¶ä½ ä¸€èµ·è¸© AI å‘</a>ï¼š<a href="https://www.twman.org/AI" target="_blank">https://www.twman.org/AI</a></h3>

<ul>
  <li>
    <b><a href="https://blog.twman.org/2025/03/AIAgent.html" target="_blank">é¿é–‹ AI Agent é–‹ç™¼é™·é˜±ï¼šå¸¸è¦‹å•é¡Œã€æŒ‘æˆ°èˆ‡è§£æ±ºæ–¹æ¡ˆ</a></b>ï¼š<a href="https://deep-learning-101.github.io/agent" target="_blank">æ¢è¨å¤šç¨® AI ä»£ç†äººå·¥å…·çš„æ‡‰ç”¨ç¶“é©—èˆ‡æŒ‘æˆ°ï¼Œåˆ†äº«å¯¦ç”¨ç¶“é©—èˆ‡å·¥å…·æ¨è–¦ã€‚</a>
  </li>
  <li>
    <b><a href="https://blog.twman.org/2024/08/LLM.html" target="_blank">ç™½è©±æ–‡æ‰‹æŠŠæ‰‹å¸¶ä½ ç§‘æ™® GenAI</a></b>ï¼š<a href="https://deep-learning-101.github.io/GenAI" target="_blank">æ·ºé¡¯ä»‹ç´¹ç”Ÿæˆå¼äººå·¥æ™ºæ…§æ ¸å¿ƒæ¦‚å¿µï¼Œå¼·èª¿ç¡¬é«”è³‡æºå’Œæ•¸æ“šçš„é‡è¦æ€§ã€‚</a>
  </li>
  <li>
    <b><a href="https://blog.twman.org/2024/09/LLM.html" target="_blank">å¤§å‹èªè¨€æ¨¡å‹ç›´æ¥å°±æ‰“å®Œæ”¶å·¥ï¼Ÿ</a></b>ï¼š<a href="https://deep-learning-101.github.io/1010LLM" target="_blank">å›é¡§ LLM é ˜åŸŸæ¢ç´¢æ­·ç¨‹ï¼Œè¨è«–ç¡¬é«”å‡ç´šå° AI é–‹ç™¼çš„é‡è¦æ€§ã€‚</a>
  </li>
  <li>
    <b><a href="https://blog.twman.org/2024/07/RAG.html" target="_blank">æª¢ç´¢å¢å¼·ç”Ÿæˆ(RAG)ä¸æ˜¯è¬éˆä¸¹ä¹‹å„ªåŒ–æŒ‘æˆ°æŠ€å·§</a></b>ï¼š<a href="https://deep-learning-101.github.io/RAG" target="_blank">æ¢è¨ RAG æŠ€è¡“æ‡‰ç”¨èˆ‡æŒ‘æˆ°ï¼Œæä¾›å¯¦ç”¨ç¶“é©—åˆ†äº«å’Œå·¥å…·å»ºè­°ã€‚</a>
  </li>
  <li>
    <b><a href="https://blog.twman.org/2024/02/LLM.html" target="_blank">å¤§å‹èªè¨€æ¨¡å‹ (LLM) å…¥é–€å®Œæ•´æŒ‡å—ï¼šåŸç†ã€æ‡‰ç”¨èˆ‡æœªä¾†</a></b>ï¼š<a href="https://deep-learning-101.github.io/0204LLM" target="_blank">æ¢è¨å¤šç¨® LLM å·¥å…·çš„æ‡‰ç”¨èˆ‡æŒ‘æˆ°ï¼Œå¼·èª¿ç¡¬é«”è³‡æºçš„é‡è¦æ€§ã€‚</a>
  </li>
  <li>
    <b><a href="https://blog.twman.org/2023/04/GPT.html" target="_blank">ä»€éº¼æ˜¯å¤§èªè¨€æ¨¡å‹ï¼Œå®ƒæ˜¯ä»€éº¼ï¼Ÿæƒ³è¦å—ï¼Ÿ(Large Language Modelï¼ŒLLM)</a></b>ï¼š<a href="https://deep-learning-101.github.io/GPU" target="_blank">æ¢è¨ LLM çš„ç™¼å±•èˆ‡æ‡‰ç”¨ï¼Œå¼·èª¿ç¡¬é«”è³‡æºåœ¨é–‹ç™¼ä¸­çš„é—œéµä½œç”¨ã€‚</a>
  </li>
  <li>
    <b><a href="https://blog.twman.org/2024/11/diffusion.html" target="_blank">Diffusion Model å®Œå…¨è§£æï¼šå¾åŸç†ã€æ‡‰ç”¨åˆ°å¯¦ä½œ (AI åœ–åƒç”Ÿæˆ)</a></b>ï¼›<a href="https://deep-learning-101.github.io/diffusion" target="_blank">æ·±å…¥æ¢è¨å½±åƒç”Ÿæˆèˆ‡åˆ†å‰²æŠ€è¡“çš„æ‡‰ç”¨ï¼Œå¼·èª¿ç¡¬é«”è³‡æºçš„é‡è¦æ€§ã€‚</a>
  </li>
  <li>
    <b><a href="https://blog.twman.org/2024/02/asr-tts.html" target="_blank">ASR/TTS é–‹ç™¼é¿å‘æŒ‡å—ï¼šèªéŸ³è¾¨è­˜èˆ‡åˆæˆçš„å¸¸è¦‹æŒ‘æˆ°èˆ‡å°ç­–</a></b>ï¼š<a href="https://deep-learning-101.github.io/asr-tts" target="_blank">æ¢è¨ ASR å’Œ TTS æŠ€è¡“æ‡‰ç”¨ä¸­çš„å•é¡Œï¼Œå¼·èª¿æ•¸æ“šè³ªé‡çš„é‡è¦æ€§ã€‚</a>
  </li>
  <li>
    <b><a href="https://blog.twman.org/2021/04/NLP.html" target="_blank">é‚£äº› NLP è¸©çš„å‘</a></b>ï¼š<a href="https://deep-learning-101.github.io/nlp" target="_blank">åˆ†äº« NLP é ˜åŸŸçš„å¯¦è¸ç¶“é©—ï¼Œå¼·èª¿æ•¸æ“šè³ªé‡å°æ¨¡å‹æ•ˆæœçš„å½±éŸ¿ã€‚</a>
  </li>
  <li>
    <b><a href="https://blog.twman.org/2021/04/ASR.html" target="_blank">é‚£äº›èªéŸ³è™•ç†è¸©çš„å‘</a></b>ï¼š<a href="https://deep-learning-101.github.io/speech" target="_blank">åˆ†äº«èªéŸ³è™•ç†é ˜åŸŸçš„å¯¦å‹™ç¶“é©—ï¼Œå¼·èª¿è³‡æ–™å“è³ªå°æ¨¡å‹æ•ˆæœçš„å½±éŸ¿ã€‚</a>
  </li>
  <li>
    <b><a href="https://blog.twman.org/2020/05/DeepLearning.html" target="_blank">æ‰‹æŠŠæ‰‹å­¸æ·±åº¦å­¸ç¿’å®‰è£ç’°å¢ƒ</a></b>ï¼š<a href="https://deep-learning-101.github.io/101" target="_blank">è©³ç´°ä»‹ç´¹åœ¨ Ubuntu ä¸Šå®‰è£æ·±åº¦å­¸ç¿’ç’°å¢ƒçš„æ­¥é©Ÿï¼Œåˆ†äº«å¯¦éš›æ“ä½œç¶“é©—ã€‚</a>
  </li>
</ul>

</details>

---

# Computer Vision (CV, é›»è…¦è¦–è¦º)

### **æ–‡ç« ç›®éŒ„**
- [Anomaly Detection](#anomalydetection)
- [Object Detection](#objectdetection)
- [Segmentation](#segmentation)
- [OCR](#ocr)
- [Diffusion model (æ“´æ•£æ¨¡å‹)](#diffusion-model-æ“´æ•£æ¨¡å‹)
- [Digital Human (è™›æ“¬æ•¸å­—äºº)](#digital-human-è™›æ“¬æ•¸å­—äºº)



## AnomalyDetection
**Anomaly Detection (ç•°å¸¸æª¢æ¸¬)**

- 2025-09-24ï½œ**FS-SAM2**
  - èªªæ˜ï¼šAdapting Segment Anything Model 2 for Few-Shot Semantic Segmentation
  - è³‡æºï¼š[ğŸ“„ AlphaXiv](https://www.alphaxiv.org/overview/2509.12105v1) | [ğŸ“ FS-SAM2 æ•ˆèƒ½èˆ‡æ•ˆç‡é›™å„ª](https://zread.ai/fornib/FS-SAM2)

- 2025-09-20ï½œ**MOCHA**
  - èªªæ˜ï¼šMulti-modal Objects-aware Cross-arcHitecture Alignment
  - è³‡æºï¼š[ğŸ“„ AlphaXiv](https://www.alphaxiv.org/zh/overview/2509.14001v1) | [ğŸ“ æ³¨å…¥ YOLO å°‘æ¨£æœ¬æª¢æ¸¬æ€§èƒ½å¤§æ¼²](https://zhuanlan.zhihu.com/p/1952054591035281418)

- 2025-07-16ï½œ**CostFilter-AD**
  - èªªæ˜ï¼šEnhancing Anomaly Detection through Matching Cost Filtering
  - è³‡æºï¼š[ğŸ™ GitHub](https://github.com/ZHE-SAPI/CostFilter-AD) | [ğŸ“ åˆ·æ–°ç„¡ç›£ç£ç•°å¸¸æª¢æ¸¬ä¸Šé™](https://zhuanlan.zhihu.com/p/1928870223529882075)

- 2025-06-13ï½œ**One-to-Normal**
  - èªªæ˜ï¼šAnomaly Personalization (å°‘æ¨£æœ¬ç•°å¸¸è­˜åˆ¥æ–°çªç ´)
  - è³‡æºï¼š[ğŸ“„ AlphaXiv](https://www.alphaxiv.org/abs/2502.01201) | [ğŸ“ æ“´æ•£æ¨¡å‹å”åŠ©ç²¾æº–åµæ¸¬](https://zhuanlan.zhihu.com/p/1916799842879018831)

- 2025-06-06ï½œ**DualAnoDiff (CVPR 2025)**
  - èªªæ˜ï¼šDual-Interrelated Diffusion Model for Few-Shot Anomaly Image Generation
  - è³‡æºï¼š[ğŸ“„ AlphaXiv](https://www.alphaxiv.org/abs/2408.13509v3) | [ğŸ“ å¾©æ—¦é¨°è¨Šå„ªåœ–æ–°ç®—æ³•](https://www.qbitai.com/2025/06/291359.html)

- 2025-05-15ï½œ**AdaptCLIP**
  - èªªæ˜ï¼šAdapting CLIP for Universal Visual Anomaly Detection
  - è³‡æºï¼š[ğŸ™ GitHub](https://github.com/aiiu-lab/AdaptCLIP) | [ğŸ“„ AlphaXiv](https://www.alphaxiv.org/overview/2407.15795) | [ğŸ“ é¨°è¨Šé–‹æºåˆ·æ–° SOTA](https://mp.weixin.qq.com/s/w5x6T18aSZt9jxqMIdf-Yg)

- 2025-05-05ï½œ**Multi-Modal LLM for AD**
  - èªªæ˜ï¼šDetect, Classify, Act: Categorizing Industrial Anomalies
  - è³‡æºï¼š[ğŸ“„ AlphaXiv](https://www.alphaxiv.org/zh/overview/2505.02626) | [ğŸ“š DeepWiki](https://deepwiki.com/Sassanmtr/VELM) | [ğŸ’¾ MVTec Dataset](https://www.mvtec.com/company/research/datasets/mvtec-ad)

- 2025-04-27ï½œ**AnomalyCLIP**
  - èªªæ˜ï¼šObject-agnostic Prompt Learning for Zero-shot Anomaly Detection
  - è³‡æºï¼š[ğŸ“„ AlphaXiv](https://www.alphaxiv.org/overview/2310.18961) | [ğŸ“š DeepWiki](https://deepwiki.com/zqhang/AnomalyCLIP)

- 2025-04-26ï½œ**PaDim**
  - èªªæ˜ï¼šç¶“å…¸ç„¡ç›£ç£ç•°å¸¸æª¢æ¸¬æ–¹æ³•
  - è³‡æºï¼š[ğŸ“„ AlphaXiv](https://www.alphaxiv.org/zh/overview/2011.08785) | [ğŸ“š DeepWiki](https://deepwiki.com/xiahaifeng1995/PaDiM-Anomaly-Detection-Localization-master)

- 2025-04-12ï½œ**AA-CLIP**
  - èªªæ˜ï¼šEnhancing Zero-shot Anomaly Detection via Anomaly-Aware CLIP
  - è³‡æºï¼š[ğŸ“„ AlphaXiv](https://www.alphaxiv.org/zh/overview/2503.06661) | [ğŸ“š DeepWiki](https://deepwiki.com/Mwxinnn/AA-CLIP)

- 2025-03-25ï½œ**Dinomaly**
  - èªªæ˜ï¼šThe Less Is More Philosophy in Multi-Class Unsupervised AD
  - è³‡æºï¼š[ğŸ™ GitHub](https://github.com/guojiajeremy/Dinomaly) | [ğŸ“ ç„¡ç›£ç£ç•°å¸¸æª¢æ¸¬ UAD è§£è®€](https://zhuanlan.zhihu.com/p/1886364053259146390)

---

## ObjectDetection
**Object Detection (ç›®æ¨™åµæ¸¬)**

- 2025ï½œ**MCL (AAAI 2025)**
  - èªªæ˜ï¼šMulti-clue Consistency Learning (é™æ„ŸåŠç›£ç£ç›®æ¨™æª¢æ¸¬)
  - è³‡æºï¼š[ğŸ“„ AlphaXiv](https://www.alphaxiv.org/abs/2407.05909) | [ğŸ™ GitHub](https://github.com/facias914/sood-mcl) | [ğŸ“ ä¸­æ–‡è§£è®€](https://zhuanlan.zhihu.com/p/26788012528)

- 2025-07-24ï½œ**OV-DINO**
  - èªªæ˜ï¼šé–‹æºå·¥æ¥­é–‹æ”¾è©å½™ç›®æ¨™æª¢æ¸¬
  - è³‡æºï¼š[ğŸ™ GitHub](https://github.com/wanghao9610/OV-DINO) | [ğŸ“ ä¸­æ–‡è§£è®€](https://mp.weixin.qq.com/s/gLAVYFAH_39gT4XC0zWN0A)

- 2025-06-18ï½œ**CountVid**
  - èªªæ˜ï¼šOpen-World Object Counting in Videos (å½±ç‰‡ä¸­æŒ‡å“ªæ•¸å“ª)
  - è³‡æºï¼š[ğŸ“„ AlphaXiv](https://www.alphaxiv.org/abs/2506.15368) | [ğŸ“ ç‰›æ´¥å¤§å­¸é–‹æº](https://mp.weixin.qq.com/s/hICrrfEgriyktoIxnbjPEQ)

- 2025-06-15ï½œ**GeoPix**
  - èªªæ˜ï¼šåƒç´ ç´šé™æ„Ÿå¤šæ¨¡æ…‹å¤§æ¨¡å‹
  - è³‡æºï¼š[ğŸ™ GitHub](https://github.com/Norman-Ou/GeoPix) | [ğŸ“ åŒ—å¤§å¯¦é©—å®¤ä»‹ç´¹](https://3slab.pku.edu.cn/info/1026/2121.htm)

- 2025-05-23ï½œ**VisionReasoner**
  - èªªæ˜ï¼šç”¨å¼·åŒ–å­¸ç¿’çµ±ä¸€è¦–è¦ºæ„ŸçŸ¥èˆ‡æ¨ç† (å°æ¨™ Qwen2.5-VL)
  - è³‡æºï¼š[ğŸ™ GitHub](https://github.com/dvlab-research/VisionReasoner) | [ğŸ“ ä¸­æ–‡è§£è®€](https://mp.weixin.qq.com/s/vECz3i_-dzvlDr3BdRLPWQ)

- 2025-03-14ï½œ**Falcon**
  - èªªæ˜ï¼šA Remote Sensing Vision-Language Foundation Model
  - è³‡æºï¼š[ğŸ“„ AlphaXiv](https://www.alphaxiv.org/abs/2503.11070) | [ğŸ“š DeepWiki](https://deepwiki.com/TianHuiLab/Falcon)

---

## Segmentation
**Segmentation (åœ–åƒåˆ†å‰²)**

- **Perceive Anything Model**
  - èªªæ˜ï¼šRecognize, Explain, Caption, and Segment Anything (å°æ¨™ SAM2 + LLM)
  - è³‡æºï¼š[ğŸ“„ AlphaXiv](https://www.alphaxiv.org/zh/overview/2506.05302v1) | [ğŸ“ ä¸­æ–‡è§£è®€](https://zhuanlan.zhihu.com/p/1919709726209446971)

- **RemoteSAM**
  - èªªæ˜ï¼šTowards Segment Anything for Earth Observation
  - è³‡æºï¼š[ğŸ“„ AlphaXiv](https://www.alphaxiv.org/abs/2505.18022v3) | [ğŸ“š DeepWiki](https://deepwiki.com/1e12Leon/RemoteSAM)

- **InstructSAM**
  - èªªæ˜ï¼šTraining-Free Framework for Remote Sensing
  - è³‡æºï¼š[ğŸŒ Project](https://voyagerxvoyagerx.github.io/InstructSAM/) | [ğŸ“„ AlphaXiv](https://www.alphaxiv.org/zh/overview/2505.15818v1) | [ğŸ“š DeepWiki](https://deepwiki.com/VoyagerXvoyagerx/InstructSAM)

- **RESAnything**
  - èªªæ˜ï¼šAttribute Prompting for Arbitrary Referring Segmentation
  - è³‡æºï¼š[ğŸ“„ AlphaXiv](https://www.alphaxiv.org/abs/2505.02867) | [ğŸŒ Project](https://suikei-wang.github.io/RESAnything/)

- **CVPR 2025 Highlights**
  - **SegAnyMo**: [Segment Any Motion in Videos](https://www.alphaxiv.org/zh/overview/2503.22268) | [ğŸ™ GitHub](https://github.com/nnanhuang/SegAnyMo)
  - **Exact**: [é™æ„Ÿå½±åƒæ™‚é–“åºåˆ—å¼±ç›£ç£å­¸ç¿’](https://zhuanlan.zhihu.com/p/38754229963) | [ğŸ™ GitHub](https://github.com/MiSsU-HH/Exact)

- **MatAnyone**
  - èªªæ˜ï¼šè¦–è¨Šæ‘³åœ–ï¼Œä¸€æ¬¡æŒ‡å®šå…¨ç¨‹è¿½è¸ªï¼Œé«®çµ²ç´šé‚„åŸ
  - è³‡æºï¼š[ğŸ™ GitHub](https://github.com/pq-yang/MatAnyone) | [ğŸ“ æ©Ÿå™¨ä¹‹å¿ƒè§£è®€](https://www.jiqizhixin.com/articles/2025-04-17-27)

- **SAM 2 & Variants (åˆ†å‰²ä¸€åˆ‡ç³»åˆ—)**
  - **Meta SAM 2**: [å®˜æ–¹ç¶²ç«™](https://ai.meta.com/sam2/) | [ğŸ“ 60è¡Œç¨‹å¼ç¢¼å¾®èª¿æ•™å­¸](https://mp.weixin.qq.com/s/YfgYCzvi0cXxOFIfQvE_9w)
  - **CLIPSeg**: [HuggingFace Space](https://huggingface.co/spaces/taesiri/CLIPSeg) | [ğŸ™ GitHub](https://github.com/timojl/clipseg)
  - **SAMURAI**: [Project](https://yangchris11.github.io/samurai/) | [ğŸ“ KF+SAM2 è§£æ±ºå¿«é€Ÿç§»å‹•/è‡ªé®æ“‹](https://mp.weixin.qq.com/s/iU3Bk_uO01GWUxAtIBsrWQ)
  - **Grounded SAM 2**: [ğŸ™ GitHub](https://github.com/IDEA-Research/Grounded-SAM-2) | [ğŸ¤— Demo](https://huggingface.co/spaces/yizhangliu/Grounded-Segment-Anything)
  - **SAM2Long**: [ğŸ™ GitHub](https://github.com/Mark12Ding/SAM2Long) | [ğŸ“ æ¸¯ä¸­æ–‡æå‡ºè¤‡é›œé•·è¦–é »åˆ†å‰²](https://mp.weixin.qq.com/s/henvaxGoNgx24NLQV1Qj2w)
  - **SAM2-Adapter**: [ğŸ™ GitHub](https://github.com/tianrun-chen/SAM-Adapter-PyTorch) | [ğŸ“ è®“ SAM 2 é©æ‡‰ä¸‹æ¸¸ä»»å‹™](https://mp.weixin.qq.com/s/3z-LshKAgbSzNCzyoLOuag)
  - **SAM2Point**: [ğŸ™ GitHub](https://github.com/ZiyuGuo99/SAM2Point) | [ğŸ“ å¯æç¤º 3D åˆ†å‰²é‡Œç¨‹ç¢‘](https://mp.weixin.qq.com/s/TnTK5UE7O_hcrNzloxBmAw)

## OCR
**Optical Character Recognition (å…‰å­¸æ–‡å­—è­˜åˆ¥)**
**[é‡å°ç‰©ä»¶æˆ–å ´æ™¯å½±åƒé€²è¡Œåˆ†æèˆ‡åµæ¸¬](https://www.twman.org/AI/CV)**

- [ä½¿ç”¨é–‹æºæ¨¡å‹å¼·åŒ–æ‚¨çš„ OCR å·¥ä½œæµç¨‹](https://huggingface.co/blog/zh/ocr-open-models)
- [12å€‹æµè¡Œçš„é–‹æºå…è²»OCRé …ç›®](https://mp.weixin.qq.com/s/7EuhnQedAX6injBL_Dg_sQ)

- 2025-11-30ï½œ**HunyuanOCR**
  - è³‡æºï¼š[ğŸ™ GitHub](https://github.com/Tencent-Hunyuan/HunyuanOCR) | [ğŸ“ é¨°è¨Šæ··å…ƒ 1B ç´šå…¨èƒ½æ¨¡å‹](https://zhuanlan.zhihu.com/p/1977498008712131326)

- 2025-10-21 | **Chandra OCR**
  - è³‡æºï¼š[ğŸ™ GitHub](https://github.com/datalab-to/chandra) | [ğŸ“ è¶…è¶ŠDeepSeek-OCRï¼ OCRé ˜åŸŸçš„é©å‘½æ€§çªç ´ï¼šChandra OCRæœ¬åœ°éƒ¨ç½²+çœŸå¯¦è©•æ¸¬](https://zhuanlan.zhihu.com/p/1969019468937144099)

- 2025-10-19ï½œ**PaddleOCR-VL**
  - è³‡æºï¼š[ğŸ¤— HuggingFace](https://huggingface.co/PaddlePaddle/PaddleOCR-VL) | [ğŸ“ åœ–ç‰‡è¾¨è­˜è½‰æ–‡å­—å·”å³°ä¹‹ä½œ](https://zhuanlan.zhihu.com/p/1964600336103745187)

- 2025-08-18ï½œ**DianJin-OCR-R1**
  - è³‡æºï¼š[ğŸ™ GitHub](https://github.com/aliyun/qwen-dianjin) | [ğŸ“ é»é‡‘ OCR-R1ï¼šæ¨¡ç³Šè“‹ç« ã€è·¨é è¡¨æ ¼å…¨æ‹¿ä¸‹](https://mp.weixin.qq.com/s/cOo0sqwDt3ARid70wBaYVA)

- 2025-07-30ï½œ**dots.ocr**
  - è³‡æºï¼š[ğŸ¤— HuggingFace](https://huggingface.co/rednote-hilab/dots.ocr) | [ğŸ“ æœ¬åœ°éƒ¨ç½² 1.7B è¶…å¼· OCR](https://zhuanlan.zhihu.com/p/1935120171573413613)

- 2025-06-16ï½œ**OCRFlux**
  - èªªæ˜ï¼šåŸºæ–¼ LLM çš„è¤‡é›œä½ˆå±€èˆ‡è·¨é åˆä½µ PDF è§£æ
  - è³‡æºï¼š[ğŸ™ GitHub](https://github.com/chatdoc-com/OCRFlux) | [ğŸŒ Demo](https://ocrflux.pdfparser.io/#/)

- 2025-06-05ï½œ**MonkeyOCR**
  - è³‡æºï¼š[ğŸ“š DeepWiki](https://deepwiki.com/Yuliang-Liu/MonkeyOCR) | [ğŸ“„ AlphaXiv](https://www.alphaxiv.org/overview/2506.05218)

  - 2025-03-05ï½œ**OpenOCR**
  - è³‡æºï¼š[ğŸ™ GitHub](https://github.com/Topdu/OpenOCR) | [ğŸ“ é€šç”¨OCRå·¥å…·OpenOCRé–‹æº
](https://zhuanlan.zhihu.com/p/10259507246)

- 2025-03-05ï½œ**PP-DocBee**
  - è³‡æºï¼š[ğŸ™ GitHub](https://github.com/PaddlePaddle/PaddleMIX/tree/develop/deploy/ppdocbee) | [ğŸ“ ç™¾åº¦æ–‡æª”å½±åƒç†è§£](https://zhuanlan.zhihu.com/p/28715553656)

- 2025-03-03ï½œ**olmocr**
  - è³‡æºï¼š[ğŸ™ GitHub](https://github.com/allenai/olmocr) | [ğŸ“ æœ¬åœ°éƒ¨ç½²ç²¾æº–æå– PDF](https://www.aivi.fyi/llms/deploy-olmOCR)

- 2025-02-05ï½œ**MinerU**
  - è³‡æºï¼š[ğŸ™ GitHub](https://github.com/opendatalab/MinerU) | [ğŸ“ PDF è½‰ Markdown ç¥å™¨](https://mp.weixin.qq.com/s/ci5wp6gICTCtaRZfn5yWUQ)

- 2024-12-15ï½œ**markitdown**
  - è³‡æºï¼š[ğŸ™ GitHub](https://github.com/microsoft/markitdown)

- 2024-10-29ï½œ**OmniParser**
  - è³‡æºï¼š[ğŸ™ GitHub](https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/OmniParser) | [ğŸ“ Alibaba å‡ºå“ï¼šé€šç”¨æ–‡æª”è¤‡é›œå ´æ™¯æŠ½å–](https://mp.weixin.qq.com/s/_1Aatpna7poIVRhfYk4aAQ)

- 2024-09-11ï½œ**GOT-OCR-2.0**
  - è³‡æºï¼š[ğŸ“ æ¨¡å‹é–‹æºä»‹ç´¹](https://mp.weixin.qq.com/s/rQL-Q0TGhT6e8Ti4zZalrg) | [ğŸ“ OCR 2.0 æ™‚ä»£ä¾†äº†](https://mp.weixin.qq.com/s/W-Ult-F3pU6Wvx3fHEN8yA)

- 2024-08-20ï½œ**PDF è½‰ MarkDown å·¥å…·**
  - è³‡æºï¼š[ğŸ“ è¬ç‰©çš†å¯ AI åŒ–ï¼12000 äººåœè§€çš„é–‹æºå·¥å…·](https://www.53ai.com/news/MultimodalLargeModel/2024082059736.html)

- **å…¶ä»–å¯¦ç”¨å·¥å…·èˆ‡è³‡æº**
  - **RapidOCR**ï¼š[ğŸ™ GitHub](https://github.com/RapidAI/RapidOCR/blob/main/docs/README_zh.md)
  - **TableStructureRec**ï¼š[ğŸ™ GitHub](https://github.com/RapidAI/TableStructureRec) | [ğŸ“ è¡¨æ ¼çµæ§‹è¾¨è­˜æ¨ç†åº«](https://zhuanlan.zhihu.com/p/668484933)
  - **PaddleOCR æ•™å­¸**ï¼š[ğŸ“ ç”¨ PPOCRLabel å¾®èª¿é†«ç™‚è¨ºæ–·æ›¸å’Œæ”¶æ“š](https://blog.twman.org/2023/07/wsl.html)


## Diffusion Model
**Diffusion Model (æ“´æ•£æ¨¡å‹)**


- 2025-05-28ï½œ**Jodi**
  - èªªæ˜ï¼šè¦–è¦ºç†è§£ & ç”Ÿæˆå¤§ä¸€çµ±æ¨¡å‹
  - è³‡æºï¼š[ğŸ“„ AlphaXiv](https://www.alphaxiv.org/zh/overview/2505.19084) | [ğŸŒ Project](https://vipl-genun.github.io/Project-Jodi/)

- 2025-05-27ï½œ**AnomalyAny (CVPR 2025)**
  - èªªæ˜ï¼šStable Diffusion å”åŠ©è¦–è¦ºç•°å¸¸æª¢æ¸¬ï¼Œç„¡éœ€è¨“ç·´
  - è³‡æºï¼š[ğŸŒ Project](https://hansunhayden.github.io/AnomalyAny.github.io/) | [ğŸ“ ä¸­æ–‡è§£è®€](https://zhuanlan.zhihu.com/p/1910284073231942689)

- 2025-05-23ï½œ**HivisionIDPhotos**
  - èªªæ˜ï¼šæ™ºæ…§è­‰ä»¶ç…§ç”Ÿæˆç¥å™¨ (æ‘³åœ–ã€æ›èƒŒæ™¯ã€ä»»æ„å°ºå¯¸)
  - è³‡æºï¼š[ğŸ“š DeepWiki](https://deepwiki.com/Zeyi-Lin/HivisionIDPhotos) | [ğŸ“ æ•™å­¸æ–‡ç« ](https://zhuanlan.zhihu.com/p/718725351)

- 2025-05-19ï½œ**Index-AniSora**
  - èªªæ˜ï¼šB ç«™é–‹æº SOTA å‹•ç•«å½±ç‰‡ç”Ÿæˆæ¨¡å‹
  - è³‡æºï¼š[ğŸ“„ AlphaXiv](https://www.alphaxiv.org/overview/2504.10044) | [ğŸ“š DeepWiki](https://deepwiki.com/bilibili/Index-anisora) | [ğŸ“ ä¸­æ–‡è§£è®€](https://zhuanlan.zhihu.com/p/1908150671540224717)

- 2025-04-26ï½œ**Insert Anything**
  - è³‡æºï¼š[ğŸ“„ AlphaXiv](https://www.alphaxiv.org/zh/overview/2504.15009) | [ğŸ“š DeepWiki](https://deepwiki.com/song-wensong/insert-anything)

- 2025-04-24ï½œ**Phantom**
  - èªªæ˜ï¼šå­—ç¯€è·³å‹• 1280x720 å½±ç‰‡ç”Ÿæˆæ¨¡å‹ï¼Œ10G é¡¯å­˜å¯ç”¨
  - è³‡æºï¼š[ğŸ™ GitHub](https://github.com/Phantom-video/Phantom) | [ğŸ“ å¯¦æ¸¬å ±å‘Š](https://zhuanlan.zhihu.com/p/1898688574477545694)

- 2025-04-22ï½œ**MAGI-1**
  - èªªæ˜ï¼šSand AI å…¨çƒé¦–å€‹è‡ªå›æ­¸å½±ç‰‡ç”Ÿæˆå¤§æ¨¡å‹
  - è³‡æºï¼š[ğŸ™ GitHub](https://github.com/SandAI-org/Magi-1) | [ğŸ“ æ€§èƒ½äº®é»è§£æ](https://www.zhihu.com/question/1898030232184795448)

- 2025-04-22ï½œ**SkyReels V2**
  - èªªæ˜ï¼šå…¨çƒé¦–å€‹ç„¡é™æ™‚é•·å½±ç‰‡ç”Ÿæˆï¼Œé›»å½±ç´šç†è§£
  - è³‡æºï¼š[ğŸ™ GitHub](https://github.com/SkyworkAI/SkyReels-V2) | [ğŸ“ åª’é«”å ±å°](https://www.qbitai.com/2025/04/275531.html)

- 2025-04-14ï½œ**FramePack**
  - èªªæ˜ï¼šComfyUI æ’ä»¶ï¼Œ6G é¡¯å­˜è·‘ 13B æ¨¡å‹ï¼Œæ”¯æ´ 1 åˆ†é˜å½±ç‰‡
  - è³‡æºï¼š[ğŸ™ GitHub](https://github.com/kijai/ComfyUI-FramePackWrapper) | [ğŸ“ æ€§åƒ¹æ¯”åˆ†æ](https://zhuanlan.zhihu.com/p/1896487969470251546)

- 2025-04-14ï½œ**Fantasy-talking**
  - èªªæ˜ï¼šåŸºæ–¼ Wan2.1 çš„éŸ³è¨Šé©…å‹•æ•¸å­—äºº
  - è³‡æºï¼š[ğŸŒ Project](https://fantasy-amap.github.io/fantasy-talking/) | [ğŸ“ è§£è®€æ–‡ç« ](https://zhuanlan.zhihu.com/p/1892895916354148118)

- 2025-04-05ï½œ**SkyReels-A2**
  - è³‡æºï¼š[ğŸ“„ AlphaXiv](https://www.alphaxiv.org/zh/overview/2504.02436) | [ğŸ“š DeepWiki](https://deepwiki.com/SkyworkAI/SkyReels-A2) | [ğŸ“ ä¸­æ–‡è§£è®€](https://zhuanlan.zhihu.com/p/1892709305301590652)

- 2025-03-10ï½œ**HunyuanVideo-I2V**
  - èªªæ˜ï¼šé¨°è¨Šé–‹æºåœ–ç”Ÿè¦–è¨Šæ¨¡å‹ + LoRA è¨“ç·´è…³æœ¬
  - è³‡æºï¼š[ğŸ™ GitHub](https://github.com/Tencent/HunyuanVideo-I2V) | [ğŸ“ å¯¦æˆ°æ•™å­¸](https://zhuanlan.zhihu.com/p/29110060025)

- 2025-02-25ï½œ**Wan-Video**
  - èªªæ˜ï¼šé˜¿é‡Œè¬ç›¸å¤§æ¨¡å‹é–‹æºï¼Œå…¨æ¨¡æ…‹ã€å…¨å°ºå¯¸
  - è³‡æºï¼š[ğŸ™ GitHub](https://github.com/Wan-Video/Wan2.1) | [ğŸ“ åª’é«”å ±å°](https://finance.sina.com.cn/jjxw/2025-02-26/doc-inemukxr9127437.shtml)

- 2025-02-14ï½œ**FlashVideo**
  - èªªæ˜ï¼šå­—ç¯€è·³å‹•è¦–è¨Šå¢å¼·æ¼”ç®—æ³•ï¼Œ102 ç§’ç”Ÿæˆ 1080P å½±ç‰‡
  - è³‡æºï¼š[ğŸ™ GitHub](https://github.com/FoundationVision/FlashVideo) | [ğŸ“ è§£è®€æ–‡ç« ](https://zhuanlan.zhihu.com/p/23702953115)

- 2025-01-28ï½œ**Sana (ICLR 2025 Oral)**
  - èªªæ˜ï¼šè‹±å‰é”/MIT/æ¸…è¯é–‹æºï¼Œæ¯” FLUX å¿« 100 å€
  - è³‡æºï¼š[ğŸ™ GitHub](https://github.com/NVlabs/Sana) | [ğŸ“ ä¸­æ–‡è§£è®€](https://zhuanlan.zhihu.com/p/19489214543)

- **Flux & Ecosystem**
  - **Flux Models**: [ğŸ¤— Black Forest Labs](https://huggingface.co/black-forest-labs)
    - [Canny-dev](https://huggingface.co/spaces/black-forest-labs/FLUX.1-Canny-dev) | [Depth-dev](https://huggingface.co/spaces/black-forest-labs/FLUX.1-Depth-dev) | [Fill-dev](https://huggingface.co/spaces/black-forest-labs/FLUX.1-Fill-dev) | [Redux-dev](https://huggingface.co/spaces/black-forest-labs/FLUX.1-Redux-dev)
  - **PuLID (2024-11-29)**: [ğŸ™ GitHub](https://github.com/ToTheBeginning/PuLID) | [ğŸ“ ComfyUI æ•™å­¸](https://mp.weixin.qq.com/s/07BMFHaSasl7-PFtkN6_Zg)
  - **Leffa (2024-12-17)**: [ğŸ™ GitHub](https://github.com/franciszzj/Leffa) | [ğŸ“ Meta AI äººç‰©ç‰¹å¾µä¿æŒ](https://juejin.cn/post/7449325873725276196)
  - **MagicQuill (2024-11-26)**: [ğŸ™ GitHub](https://github.com/magic-quill/MagicQuill) | [ğŸ¤— Space](https://huggingface.co/spaces/AI4Editing/MagicQuill) | [ğŸ“ AI P åœ–ç¥å™¨](https://mp.weixin.qq.com/s/Pc3xRP8_9BxkVSRNznkplw)

- **Practical Tools (ComfyUI & Others)**
  - **OOTDiffusion**: [ğŸ™ GitHub](https://github.com/levihsu/OOTDiffusion) | [ğŸ“ AI æ›è£ç¥å™¨](https://mp.weixin.qq.com/s/B2rNCjJLo8coYzoHGPnVaw)
  - **ComfyUI Impact Pack**: [ğŸ™ GitHub](https://github.com/ltdrdata/ComfyUI-Impact-Pack) | [ğŸ“ æœ€å¼·è‡‰éƒ¨ä¿®å¾©](https://mp.weixin.qq.com/s/hNQ9BfdGbRQ_Osus-yMJWg)
  - **OmniGen**: [ğŸ™ GitHub](https://github.com/AIFSH/OmniGen-ComfyUI) | [ğŸ“ å…¨èƒ½å½±åƒç”Ÿæˆ](https://mp.weixin.qq.com/s/msGK0FmNs3T3jbUBHfR9DA)

---

## Digital Human
**Digital Human (è™›æ“¬æ•¸å­—äºº)**

- **Open Avatar Chat**
  - è³‡æºï¼š[ğŸ“ å°ˆæ¡ˆä»‹ç´¹](https://zread.ai/HumanAIGC-Engineering/OpenAvatarChat) | [ğŸ“ GitHub çˆ†ç«ç¥å™¨ï¼Œæœ¬åœ°éƒ¨ç½²ç„¡å¥—è·¯](https://mp.weixin.qq.com/s/eNRbU4lZLgdpe_iNSNcfGA)

- **HeyGem**
  - è³‡æºï¼š[ğŸ™ GitHub](https://github.com/GuijiAI/HeyGem.ai) | [ğŸ“ æ•¸å­—äººå…‹éš†ç¥å™¨](https://zhuanlan.zhihu.com/p/29274862393)

- **Duix**
  - è³‡æºï¼š[ğŸ™ GitHub](https://github.com/GuijiAI/duix.ai) | [ğŸ“ å…¨çƒé¦–å€‹çœŸäººæ•¸å­—äººé–‹æº](https://zhuanlan.zhihu.com/p/716583514)

- **Linly-Talker**
  - èªªæ˜ï¼šçµåˆ LLM èˆ‡è¦–è¦ºæ¨¡å‹çš„æ™ºèƒ½äº¤äº’ç³»çµ±
  - è³‡æºï¼š[ğŸ™ GitHub](https://github.com/Kedreamix/Linly-Talker)

- **CVPR 2025 / NeurIPS Resources**
  - **EchoMimicV2 (CVPR 2025)**: [ğŸ™ GitHub](https://github.com/antgroup/echomimic_v2) - Striking, Simplified Human Animation.
  - **Hallo3 (CVPR 2025)**: [ğŸ™ GitHub](https://github.com/fudan-generative-vision/hallo3) - Highly Dynamic Portrait Animation.
  - **MimicTalk (NeurIPS 2024)**: [ğŸ™ GitHub](https://github.com/yerfor/MimicTalk) - 3D talking face.

- **Other Tools**
  - **JoyGen**: [ğŸ™ GitHub](https://github.com/JOY-MM/JoyGen) (Audio-Driven 3D Editing)
  - **Latentsync**: [ğŸ™ GitHub](https://github.com/bytedance/LatentSync)
  - **MuseTalk**: [ğŸ™ GitHub](https://github.com/TMElyralab/MuseTalk)

---

## Image Recognition
**Image Recognition (åœ–åƒè­˜åˆ¥)**


- **ViT (Vision Transformer)**
  - è³‡æºï¼š[ğŸ™ GitHub](https://github.com/google-research/vision_transformer) | [ğŸ“ è§£ææ–‡ç« ](https://zhuanlan.zhihu.com/p/445122996) | [ğŸ“ é·ç§»è¡¨ç¾åˆ†æ](https://zhuanlan.zhihu.com/p/463608959)

- **Swin Transformer**
  - è³‡æºï¼š[ğŸ™ GitHub](https://github.com/microsoft/Swin-Transformer) | [ğŸ“ ç”¨ CNN æ–¹å¼æ‰“æ•— CNN](https://zhuanlan.zhihu.com/p/362690149)

- **EfficientNetV2**
  - è³‡æºï¼š[ğŸ™ GitHub](https://github.com/d-li14/efficientnetv2.pytorch) | [ğŸ“ æ›´å°æ›´å¿«çš„è¨“ç·´](https://zhuanlan.zhihu.com/p/361873583)

---

## Document AI
**Document Understanding & OCR (æ–‡æª”ç†è§£èˆ‡æ–‡å­—è­˜åˆ¥)**

- **Donut (2022)**: OCR-free Document Understanding Transformer. [ğŸ“„ arXiv:2111.15664](./donut.md)
- **LayoutParser (2021)**: Unified toolkit for Deep Learning Based Document Analysis. [ğŸ“„ arXiv:2103.15348](./LayoutParser.md)
- **TrOCR (2021)**: Transformer-based OCR with Pre-trained Models. [ğŸ“„ arXiv:2109.10282](./TrOCR.md)
- **DiT (2022)**: Self-supervised Pre-training for Document Image Transformer. [ğŸ“„ arXiv:2203.02378](./DiT.md)
- **Nougat (2023)**: Neural Optical Understanding for Academic Documents. [ğŸ“„ arXiv:2308.13418](https://facebookresearch.github.io/nougat/)

<details>
<summary><strong>ğŸ“š LayoutLM Series (é»æ“Šå±•é–‹)</strong></summary>

- **LayoutLM (2020)**: Pre-training of Text and Layout. [ğŸ“„ arXiv:1912.13318](./LayoutLM.md)
- **LayoutLMv2 (2021)**: Multi-modal Pre-training. [ğŸ“„ arXiv:2012.14740](./LayoutLMv2.md)
- **LayoutXLM (2021)**: Multilingual Visually-rich Document Understanding. [ğŸ“„ arXiv:2104.08836](./LayoutXLM.md)
- **LayoutLMv3 (2022)**: Pre-training with Unified Text and Image Masking. [ğŸ“„ arXiv:2204.08387](./LayoutLMv3.md)
</details>

- **Scene Text Recognition**
  - **ABINet (2021)**: Read Like Humans. [ğŸ“„ arXiv:2103.06495](./ABINet.md)
  - **ABINet++ (2022)**: Iterative Language Modeling for Text Spotting. [ğŸ“„ arXiv:2211.10578](./ABINet%2B%2B.md)
  - **ABCNet v2 (2021)**: Adaptive Bezier-Curve Network. [ğŸ“„ arXiv:2105.03620](./ABCNet_v2.md)
  - **SVTR (2022)**: Scene Text Recognition with a Single Visual Model. [ğŸ“„ arXiv:2205.00159](./SVTR.md)

---

## DeepFake Detection
**DeepFake Detection (æ·±åº¦å½é€ åµæ¸¬)**

- **Multi-attentional Deepfake Detection (CVPR 2021)**
  - H. Zhao et al., Proceedings of the IEEE/CVF CVPR 2021.

- **Geometric Features (CVPR 2021)**
  - Improving Efficiency and Robustness through Precise Geometric Features. Sun, Zekun et al.

- **3D Decomposition (CVPR 2021)**
  - Face Forgery Detection by 3D Decomposition. Xiangyu Zhu et al.
